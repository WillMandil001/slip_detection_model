{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "exposed-review",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from string import digits\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "seed = 42\n",
    "epochs = 20\n",
    "batch_size = 32\n",
    "learning_rate = 1e-3\n",
    "context_frames = 10\n",
    "sequence_length = 20\n",
    "lookback = sequence_length\n",
    "\n",
    "context_epochs = 20\n",
    "context_batch_size = 1\n",
    "context_learning_rate = 1e-3\n",
    "context_data_length = 20\n",
    "\n",
    "trainsize = 0.8\n",
    "valsize = 0.9\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")#  use gpu if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sought-pursuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullDataSet():\n",
    "    def __init__(self, chopmin, chopmax, testset=False, dataset=False):\n",
    "        if not testset:\n",
    "            dataset_full = []\n",
    "            for value in data_map[1:]:  # ignore header\n",
    "                dataset_full.append(self.forward(value))\n",
    "            self.samples = dataset_full[int(len(dataset_full)*chopmin):int(len(dataset_full)*chopmax)]\n",
    "        if dataset:\n",
    "            self.samples = dataset\n",
    "                                    \n",
    "    def testsetreturn(self):\n",
    "        experiment = []\n",
    "        dataset_full = []\n",
    "        current_experiment = data_map[1][-2]\n",
    "        for value in data_map[1:]:  # ignore header\n",
    "            if current_experiment == value[-2]:\n",
    "                experiment.append(self.forward(value))\n",
    "            else:\n",
    "                dataset_full.append(experiment)\n",
    "                experiment = [self.forward(value)]\n",
    "                current_experiment = value[-2]\n",
    "        return dataset_full\n",
    "\n",
    "    def forward(self, value):\n",
    "        state = np.load(data_dir + value[4])\n",
    "        return [np.load(data_dir + value[8]),\n",
    "                             np.float32(np.load(data_dir + value[2])),\n",
    "                             np.float32(np.load(data_dir + value[3])),\n",
    "                             np.float32(np.asarray([state[0] for i in range(0, len(state))]))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return(self.samples[idx])\n",
    "\n",
    "data_dir = '/home/user/Robotics/Data_sets/slip_detection/vector_normalised_002/'\n",
    "with open(data_dir + 'map.csv', 'r') as f:  # rb\n",
    "    data_map = [row for row in csv.reader(f)]\n",
    "\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "train_loader = torch.utils.data.DataLoader(FullDataSet(0, trainsize), batch_size=batch_size, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(FullDataSet(trainsize, valsize), batch_size=batch_size, shuffle=True)\n",
    "test_experiments = [torch.utils.data.DataLoader(test_exp, batch_size=batch_size, shuffle=False) for test_exp in FullDataSet(0, trainsize, testset=True).testsetreturn()]\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "treated-stationery",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FullModel, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(48, 48).to(device)  # tactile\n",
    "        self.lstm2 = nn.LSTM(48, 48).to(device)  # context\n",
    "        self.fc1 = nn.Linear(96, 48)  # tactile + context\n",
    "        self.lstm3 = nn.LSTM(6, 6).to(device)  # pos_vel\n",
    "        self.fc2 = nn.Linear(54, 48)  # tactile + pos_vel\n",
    "        self.lstm4 = nn.LSTM(48, 48).to(device)  # tactile, context, robot\n",
    "\n",
    "    def forward(self, tactiles, actions, context):\n",
    "        state = actions[0]\n",
    "        state.to(device)\n",
    "        batch_size__ = tactiles.shape[1]\n",
    "        outputs = []\n",
    "        hidden1 = (torch.rand(1,batch_size__,48).to(device), torch.rand(1,batch_size__,48).to(device))\n",
    "        hidden2 = (torch.rand(1,batch_size__,48).to(device), torch.rand(1,batch_size__,48).to(device))\n",
    "        hidden3 = (torch.rand(1,batch_size__,6).to(device), torch.rand(1,batch_size__,6).to(device))\n",
    "        hidden4 = (torch.rand(1,batch_size__,48).to(device), torch.rand(1,batch_size__,48).to(device))\n",
    "        for index, [sample_tactile, sample_action, sample_context] in enumerate(zip(tactiles.squeeze(), actions.squeeze(), context.squeeze())):\n",
    "            # 2. Run through lstm:\n",
    "            if index > context_frames-1:\n",
    "                out1, hidden1 = self.lstm1(out6, hidden1)\n",
    "                out2, hidden2 = self.lstm2(sample_context.unsqueeze(0).to(device), hidden2)\n",
    "                context_and_tactile = torch.cat((out2.squeeze(), out1.squeeze()), 1)\n",
    "                out3 = self.fc1(context_and_tactile.unsqueeze(0).cpu().detach())\n",
    "\n",
    "                out4, hidden3 = self.lstm3(sample_action.unsqueeze(0).to(device), hidden3)\n",
    "                context_and_tactile_and_robot = torch.cat((out3.squeeze().to(device), out4.squeeze()), 1)\n",
    "\n",
    "                out5 = self.fc2(context_and_tactile_and_robot.unsqueeze(0).cpu().detach())\n",
    "                out6, hidden4 = self.lstm4(out5.to(device), hidden4)\n",
    "                outputs.append(out6.squeeze())\n",
    "            else:\n",
    "                out1, hidden1 = self.lstm1(sample_tactile.unsqueeze(0).to(device), hidden1)\n",
    "                out2, hidden2 = self.lstm2(sample_context.unsqueeze(0).to(device), hidden2)\n",
    "                context_and_tactile = torch.cat((out2.squeeze(), out1.squeeze()), 1)\n",
    "                out3 = self.fc1(context_and_tactile.unsqueeze(0).cpu().detach())\n",
    "\n",
    "                out4, hidden3 = self.lstm3(sample_action.unsqueeze(0).to(device), hidden3)\n",
    "                context_and_tactile_and_robot = torch.cat((out3.squeeze().to(device), out4.squeeze()), 1)\n",
    "\n",
    "                out5 = self.fc2(context_and_tactile_and_robot.unsqueeze(0).cpu().detach())\n",
    "                out6, hidden4 = self.lstm4(out5.to(device), hidden4)\n",
    "\n",
    "        return torch.stack(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "legal-placement",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12740 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 48, got 6",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-3c1528794ddf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m         tactile_predictions = full_model.forward(batch_features[1].permute(1,0,2).to(device),\n\u001b[1;32m     16\u001b[0m                                                  \u001b[0mbatch_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                                                  batch_features[3].permute(1,0,2).to(device))\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtactile_predictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtactile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcontext_frames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-8dbb6dfd6526>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tactiles, actions, context)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0mout1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_tactile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                 \u001b[0mout2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m                 \u001b[0mcontext_and_tactile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0mout3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_and_tactile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Robotics/slip_detection_model/venv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Robotics/slip_detection_model/venv/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "\u001b[0;32m~/Robotics/slip_detection_model/venv/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m         \u001b[0mexpected_hidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_expected_hidden_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Robotics/slip_detection_model/venv/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_input\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    178\u001b[0m             raise RuntimeError(\n\u001b[1;32m    179\u001b[0m                 'input.size(-1) must be equal to input_size. Expected {}, got {}'.format(\n\u001b[0;32m--> 180\u001b[0;31m                     self.input_size, input.size(-1)))\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_expected_hidden_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 48, got 6"
     ]
    }
   ],
   "source": [
    "### Train the model:\n",
    "full_model = FullModel()\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.Adam(full_model.parameters(), lr=learning_rate)\n",
    "\n",
    "early_stop_clock = 0\n",
    "plot_training_loss = [1]\n",
    "plot_validation_loss = [1]\n",
    "progress_bar = tqdm.tqdm(range(epochs), total=(epochs*len(train_loader)))\n",
    "for epoch in progress_bar:\n",
    "    train_losses = 0\n",
    "    val_losses = 0.0\n",
    "    for index, batch_features in enumerate(train_loader):\n",
    "        # 1. Reshape data and send to device:\n",
    "        tactile_predictions = full_model.forward(batch_features[1].permute(1,0,2).to(device),\n",
    "                                                 batch_features[2].permute(1,0,2).to(device),\n",
    "                                                 batch_features[3].permute(1,0,2).to(device))\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(tactile_predictions.to(device), tactile[context_frames:])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses += loss.item()\n",
    "        progress_bar.set_description(\"epoch: {}, \".format(epoch) + \"loss: {:.4f}, \".format(float(loss.item())) + \"mean loss: {:.4f}, \".format(train_losses/(index+1)))\n",
    "        progress_bar.update()\n",
    "    mean = train_losses / index + 1\n",
    "    plot_training_loss.append(mean)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for index, batch_features in enumerate(valid_loader):\n",
    "            tactile_predictions = full_model.forward(batch_features[1].permute(1,0,2).to(device),\n",
    "                                                     batch_features[2].permute(1,0,2).to(device),\n",
    "                                                     batch_features[3].permute(1,0,2).to(device))\n",
    "            optimizer.zero_grad()\n",
    "            val_loss = criterion(tactile_predictions.to(device), batch_features[1].permute(1,0,2)[context_frames:])\n",
    "            val_losses += val_loss.item()\n",
    "\n",
    "    plot_validation_loss.append(val_losses / index)\n",
    "\n",
    "    if plot_validation_loss[-2] < plot_validation_loss[-1]:\n",
    "        early_stop_clock +=1\n",
    "        if early_stop_clock == 3:\n",
    "            break\n",
    "    else:\n",
    "        early_stop_clock = 0\n",
    "\n",
    "plt.plot(plot_training_loss[1:], c=\"r\", label=\"train loss MAE\")\n",
    "plt.plot(plot_validation_loss[1:], c='b', label=\"val loss MAE\")\n",
    "plt.legend(loc=\"upper right\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elementary-annual",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test the model:\n",
    "criterion1 = nn.L1Loss()\n",
    "criterion2 = nn.MSELoss()\n",
    "tactile_predictions_full, tactile_groundtruth_full = [], []\n",
    "with torch.no_grad():\n",
    "    for test_number, test_loader in enumerate(DataSampleSet):\n",
    "        tactile_predictions, tactile_groundtruth = [], []\n",
    "        test_lossesMAE, test_lossesMSE = 0.0, 0.0\n",
    "        for index__, batch_features in enumerate(test_loader):\n",
    "            tp = model.forward(batch_features.permute(0,2,1,3).to(device))\n",
    "            tactile_predictions.append(tp)\n",
    "            tactile_groundtruth.append(batch_features[1].permute(1,0,2)[context_frames:])\n",
    "            # calculate losses\n",
    "            test_lossesMAE += criterion1(tp.to(device), batch_features[1].permute(1,0,2)[context_frames:]).item()\n",
    "            test_lossesMSE += criterion2(tp.to(device), batch_features[1].permute(1,0,2)[context_frames:]).item()\n",
    "\n",
    "        print(\"test \" + str(test_number) + \" loss MAE(L1): \", str(test_lossesMAE / index__))\n",
    "        print(\"test \" + str(test_number) + \" loss MSE: \", str(test_lossesMSE / index__))\n",
    "\n",
    "        tactile_predictions_full.append(tactile_predictions)\n",
    "        tactile_groundtruth_full.append(tactile_groundtruth)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bright-minutes",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate tactile values for full sample:\n",
    "# calculate tactile values for full sample:\n",
    "predicted_data_t1, predicted_data_t9, groundtruth_data_t1 = [], [], []\n",
    "tactile_predictions = tactile_predictions_full[experiment_to_test]\n",
    "tactile_groundtruth = tactile_groundtruth_full[experiment_to_test]\n",
    "for index, batch_set in enumerate(tactile_predictions):\n",
    "    for batch in range(0, len(batch_set[0])):\n",
    "        predicted_data_t1.append(batch_set[0][batch])\n",
    "        groundtruth_data_t1.append(tactile_groundtruth[index][0][batch])\n",
    "        predicted_data_t9.append(batch_set[5][batch])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
