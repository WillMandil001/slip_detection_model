{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "opened-surge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --user ipykernel\n",
    "# python -m ipykernel install --user --name=myenv\n",
    "import csv\n",
    "import tqdm\n",
    "import click\n",
    "import logging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from string import digits\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "\n",
    "seed = 42\n",
    "epochs = 20\n",
    "batch_size = 32\n",
    "learning_rate = 1e-3\n",
    "context_frames = 10\n",
    "sequence_length = 20\n",
    "lookback = sequence_length\n",
    "\n",
    "context_epochs = 20\n",
    "context_batch_size = 1\n",
    "context_learning_rate = 1e-3\n",
    "context_data_length = 20\n",
    "\n",
    "valid_train_split = 0.8  # precentage of train data from total\n",
    "test_train_split = 0.9  # precentage of train data from total\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")#  use gpu if available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "negative-defensive",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "class BatchGenerator:\n",
    "    def __init__(self, data_dir, logger):\n",
    "        self.data_dir = data_dir\n",
    "        data_map = []\n",
    "        with open(data_dir + 'map.csv', 'r') as f:  # rb\n",
    "            reader = csv.reader(f)\n",
    "            for row in reader:\n",
    "                data_map.append(row)\n",
    "\n",
    "        if len(data_map) <= 1: # empty or only header\n",
    "            logger.error(\"No file map found\")\n",
    "            exit()\n",
    "\n",
    "        self.data_map = data_map\n",
    "\n",
    "    def load_full_data(self):\n",
    "        dataset_train = FullDataSet(self.data_dir, self.data_map, type_=\"train\")\n",
    "        dataset_valid = FullDataSet(self.data_dir, self.data_map, type_=\"valid\")\n",
    "        dataset_test = FullDataSet(self.data_dir, self.data_map, type_=\"test\")\n",
    "        transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "        train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "        valid_loader = torch.utils.data.DataLoader(dataset_valid, batch_size=batch_size, shuffle=True)\n",
    "        test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size, shuffle=False)\n",
    "        return train_loader, valid_loader, test_loader\n",
    "\n",
    "\n",
    "class FullDataSet():\n",
    "    def __init__(self, data_dir, data_map, type_=\"train\"):\n",
    "        dataset_full = []\n",
    "        for index, value in enumerate(data_map[1:]):  # ignore header\n",
    "            state = np.float32(np.load(data_dir + '/' + value[4]))\n",
    "            dataset_full.append([np.load(data_dir + value[8]),\n",
    "                                 np.float32(np.load(data_dir + '/' + value[2])),\n",
    "                                 np.float32(np.load(data_dir + '/' + value[3])),\n",
    "                                 np.asarray([state[0] for i in range(0, len(state))])])\n",
    "        if type_ == \"train\":\n",
    "            self.samples = dataset_full[0:int(len(dataset_full)*test_train_split)]\n",
    "        elif type_ == \"valid\":\n",
    "            self.samples = dataset_full[int(len(dataset_full)*(valid_train_split)):int(len(dataset_full)*test_train_split)]\n",
    "        elif type_ == \"test\":\n",
    "            self.samples = dataset_full[int(len(dataset_full)*test_train_split):-1]\n",
    "\n",
    "        data_map = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return(self.samples[idx])\n",
    "\n",
    "data_dir = '/home/user/Robotics/Data_sets/slip_detection/vector_normalised_002/'\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "BG = BatchGenerator(data_dir, logger)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "enclosed-tulsa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FullModel, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(48, 48).to(device)  # tactile\n",
    "        self.lstm2 = nn.LSTM(6, 6).to(device)  # pos_vel\n",
    "        self.fc1 = nn.Linear(48+6, 48)  # tactile + pos_vel\n",
    "        self.lstm3 = nn.LSTM(48, 48).to(device)  # pos_vel\n",
    "\n",
    "    def forward(self, tactiles, actions):\n",
    "        state = actions[0]\n",
    "        state.to(device)\n",
    "        batch_size__ = tactiles.shape[1]\n",
    "        outputs = []\n",
    "        hidden1 = (torch.rand(1,batch_size__,48).to(device), torch.rand(1,batch_size__,48).to(device))\n",
    "        hidden2 = (torch.rand(1,batch_size__,6).to(device), torch.rand(1,batch_size__,6).to(device))\n",
    "        hidden3 = (torch.rand(1,batch_size__,48).to(device), torch.rand(1,batch_size__,48).to(device))\n",
    "        for index, (sample_tactile, sample_action) in enumerate(zip(tactiles.squeeze(), actions.squeeze())):\n",
    "            sample_tactile.to(device)\n",
    "            sample_action.to(device)\n",
    "            # 2. Run through lstm:\n",
    "            if index > context_frames-1:\n",
    "                out1, hidden1 = self.lstm1(out4, hidden1)\n",
    "                out2, hidden2 = self.lstm2(sample_action.unsqueeze(0), hidden2)\n",
    "                robot_and_tactile = torch.cat((out2.squeeze(), out1.squeeze()), 1)\n",
    "                out3 = self.fc1(robot_and_tactile.unsqueeze(0).cpu().detach())\n",
    "                out4, hidden3 = self.lstm3(out3.to(device), hidden3)\n",
    "                outputs.append(out4.squeeze())\n",
    "            else:\n",
    "                out1, hidden1 = self.lstm1(sample_tactile.unsqueeze(0), hidden1)\n",
    "                out2, hidden2 = self.lstm2(sample_action.unsqueeze(0), hidden2)\n",
    "                robot_and_tactile = torch.cat((out2.squeeze(), out1.squeeze()), 1)\n",
    "                out3 = self.fc1(robot_and_tactile.unsqueeze(0).cpu().detach())\n",
    "                out4, hidden3 = self.lstm3(out3.to(device), hidden3)\n",
    "\n",
    "        return torch.stack(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "reduced-orange",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, logger, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        ### Train the LSTM chain:\n",
    "        self.train_full_loader, self.valid_full_loader, self.test_full_loader = BG.load_full_data()\n",
    "        self.full_model = FullModel()\n",
    "        self.criterion = nn.L1Loss()\n",
    "        self.optimizer = optim.Adam(self.full_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    def train_full_model(self):\n",
    "        plot_training_loss = []\n",
    "        plot_validation_loss = []\n",
    "        previous_val_mean_loss = 1.0\n",
    "        early_stop_clock = 0\n",
    "        progress_bar = tqdm.tqdm(range(0, epochs), total=(epochs*len(self.train_full_loader)))\n",
    "        mean_test = 0\n",
    "        for epoch in progress_bar:\n",
    "            loss = 0\n",
    "            losses = 0.0\n",
    "            for index, batch_features in enumerate(self.train_full_loader):\n",
    "                tactile = batch_features[1].permute(1,0,2).to(device)\n",
    "                action = batch_features[2].permute(1,0,2).to(device)\n",
    "                state = batch_features[3].permute(1,0,2).to(device)\n",
    "                \n",
    "                tactile_predictions = self.full_model.forward(tactiles=tactile, actions=action) # Step 3. Run our forward pass.\n",
    "                self.optimizer.zero_grad()\n",
    "                loss = self.criterion(tactile_predictions.to(device), tactile[context_frames:])\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                losses += loss.item()\n",
    "                if index:\n",
    "                    mean = losses / index\n",
    "                else:\n",
    "                    mean = 0\n",
    "                progress_bar.set_description(\"epoch: {}, \".format(epoch) + \"loss: {:.4f}, \".format(float(loss.item())) + \"mean loss: {:.4f}, \".format(mean))\n",
    "                progress_bar.update()\n",
    "            plot_training_loss.append(mean)\n",
    "\n",
    "            val_losses = 0.0\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for index__, batch_features in enumerate(self.valid_full_loader):\n",
    "                    tactile = batch_features[1].permute(1,0,2).to(device)\n",
    "                    action = batch_features[2].permute(1,0,2).to(device)\n",
    "                    state = batch_features[3].permute(1,0,2).to(device)\n",
    "\n",
    "                    tactile_predictions = self.full_model.forward(tactiles=tactile, actions=action)  # Step 3. Run our forward pass.\n",
    "                    self.optimizer.zero_grad()\n",
    "                    val_loss = self.criterion(tactile_predictions.to(device), tactile[context_frames:])\n",
    "                    val_losses += val_loss.item()\n",
    "\n",
    "            print(\"Validation mean loss: {:.4f}, \".format(val_losses / index__))\n",
    "            plot_validation_loss.append(val_losses / index__)\n",
    "            if previous_val_mean_loss < val_losses / index__:\n",
    "                early_stop_clock +=1\n",
    "                if early_stop_clock == 3:\n",
    "                    print(\"Early stopping\")\n",
    "                    break\n",
    "            else:\n",
    "                previous_val_mean_loss = val_losses / index__ \n",
    "        plt.plot(plot_training_loss, c=\"r\", label=\"train loss MAE\")\n",
    "        plt.plot(plot_validation_loss, c='b', label=\"val loss MAE\")\n",
    "        plt.legend(loc=\"upper right\")\n",
    "\n",
    "MT = ModelTrainer(logger, data_dir)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "arranged-archive",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 0.0166, mean loss: 0.0190, :   5%|▌         | 727/14340 [00:13<09:40, 23.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mean loss: 0.0175, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 2, loss: 0.0095, mean loss: 0.0100, :  10%|█         | 1444/14340 [00:26<11:22, 18.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mean loss: 0.0098, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 3, loss: 0.0074, mean loss: 0.0102, :  15%|█▌        | 2161/14340 [00:40<08:35, 23.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mean loss: 0.0088, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 4, loss: 0.0075, mean loss: 0.0091, :  20%|██        | 2878/14340 [00:53<10:09, 18.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mean loss: 0.0083, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 5, loss: 0.0059, mean loss: 0.0083, :  25%|██▌       | 3595/14340 [01:07<07:34, 23.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mean loss: 0.0077, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 6, loss: 0.0066, mean loss: 0.0084, :  30%|███       | 4318/14340 [01:20<06:26, 25.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mean loss: 0.0080, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 7, loss: 0.0076, mean loss: 0.0074, :  35%|███▌      | 5035/14340 [01:33<06:37, 23.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mean loss: 0.0071, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 8, loss: 0.0064, mean loss: 0.0079, :  40%|████      | 5752/14340 [01:46<07:31, 19.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mean loss: 0.0079, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 9, loss: 0.0061, mean loss: 0.0070, :  45%|████▌     | 6469/14340 [02:00<05:29, 23.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mean loss: 0.0069, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 9, loss: 0.0061, mean loss: 0.0069, :   0%|          | 9/14340 [02:13<58:52:45, 14.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mean loss: 0.0076, \n",
      "Early stopping\n",
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "MT.train_full_model()\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "official-strike",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss MAE(L1):  0.008138017728924751\n",
      "test loss MSE:  0.00037138887380680464\n"
     ]
    }
   ],
   "source": [
    "model = MT.full_model\n",
    "data_dir = MT.data_dir\n",
    "test_loader = MT.test_full_loader\n",
    "\n",
    "data_map = []\n",
    "with open(data_dir + 'map.csv', 'r') as f:  # rb\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        data_map.append(row)\n",
    "\n",
    "if len(data_map) <= 1: # empty or only header\n",
    "    logger.error(\"No file map found\")\n",
    "    exit()\n",
    "\n",
    "# test model on the full test sample:\n",
    "criterion1 = nn.L1Loss()\n",
    "criterion2 = nn.MSELoss()\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "train_loader = torch.utils.data.DataLoader(dataset_sample, batch_size=15, shuffle=False)\n",
    "tactile_predictions = []\n",
    "tactile_groundtruth = []\n",
    "test_lossesMAE = 0.0\n",
    "test_lossesMSE = 0.0\n",
    "with torch.no_grad():\n",
    "    for index__, batch_features in enumerate(test_loader):\n",
    "        # 2. Reshape data and send to device:\n",
    "        tactile = batch_features[1].permute(1,0,2).to(device)\n",
    "        action = batch_features[2].permute(1,0,2).to(device)\n",
    "        state = batch_features[3].permute(1,0,2).to(device)\n",
    "\n",
    "        tp = model.forward(tactiles=tactile, actions=action)\n",
    "        tactile_predictions.append(tp)  # Step 3. Run our forward pass.\n",
    "        tactile_groundtruth.append(tactile[context_frames:])\n",
    "        # calculate losses\n",
    "        test_lossMAE = criterion1(tp.to(device), tactile[context_frames:])\n",
    "        test_lossesMAE += test_lossMAE.item()\n",
    "        test_lossMSE = criterion2(tp.to(device), tactile[context_frames:])\n",
    "        test_lossesMSE += test_lossMSE.item()\n",
    "\n",
    "print(\"test loss MAE(L1): \", str(test_lossesMAE / index__))\n",
    "print(\"test loss MSE: \", str(test_lossesMSE / index__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bridal-faith",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate tactile values for full sample:\n",
    "time_step_to_test_t1 = 0    # [batch_set, prediction frames(t1->tx)(6), batch_size, features(48)]\n",
    "time_step_to_test_t9 = 5\n",
    "predicted_data_t1 = []\n",
    "predicted_data_t9 = []\n",
    "groundtruth_data = []\n",
    "for index, batch_set in enumerate(tactile_predictions):\n",
    "    for batch in range(0, len(batch_set[0])):\n",
    "        prediction_values = batch_set[time_step_to_test_t1][batch]\n",
    "        predicted_data_t1.append(prediction_values)\n",
    "        prediction_values = batch_set[time_step_to_test_t9][batch]\n",
    "        predicted_data_t9.append(prediction_values)\n",
    "        gt_values = tactile_groundtruth[index][time_step_to_test_t1][batch]\n",
    "        groundtruth_data.append(gt_values)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "hybrid-exercise",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-f57fe74246a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mpredicted_taxel_t9\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m310\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m325\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m#len(predicted_data_t1)):  # add in length of context data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mpredicted_taxel_t1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_data_t1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mpredicted_taxel_t9\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_data_t9\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mgroundtruth_taxle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroundtruth_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# test data\n",
    "index = 0\n",
    "titles = [\"sheerx\", \"sheery\", \"normal\"]\n",
    "for j in range(3):\n",
    "    for i in range(16):\n",
    "        groundtruth_taxle = []\n",
    "        predicted_taxel = []\n",
    "        predicted_taxel_t1 = []\n",
    "        predicted_taxel_t9 = []\n",
    "        for k in range(310, 325):#len(predicted_data_t1)):  # add in length of context data\n",
    "            predicted_taxel_t1.append(predicted_data_t1[k][j+i].cpu().detach().numpy())\n",
    "            predicted_taxel_t9.append(predicted_data_t9[k][j+i].cpu().detach().numpy())\n",
    "            groundtruth_taxle.append(groundtruth_data[k][j+i].cpu().detach().numpy())\n",
    "\n",
    "        index += 1\n",
    "        plt.title(\"Simple_LSTM\")\n",
    "        plt.plot(predicted_taxel_t1, alpha=0.5, c=\"b\", label=\"t5\")\n",
    "        plt.plot(predicted_taxel_t9, alpha=0.5, c=\"g\", label=\"t0\")\n",
    "        plt.plot(groundtruth_taxle, alpha=0.5, c=\"r\", label=\"gt\")\n",
    "        plt.ylim([0, 1])\n",
    "        plt.grid()\n",
    "        plt.legend(loc=\"upper right\")\n",
    "#         plt.savefig('/home/user/Robotics/slip_detection_model/images/Simple_LSTM/simple_model_test_sample_' + str(index) + '.png')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original-strategy",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
