{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "southeast-policy",
   "metadata": {},
   "source": [
    "Hello :D \n",
    "\n",
    "**edit the data_dir and out_dir for you're specific PC.**\n",
    "\n",
    "**data_dir is where the data is stored**\n",
    "\n",
    "**out_dit is where to save the trained model and images.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binding-telephone",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "### RUN IN PYTHON 3\n",
    "import os\n",
    "import cv2\n",
    "import csv\n",
    "import glob\n",
    "import click\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image \n",
    "from tqdm import tqdm\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from scipy.ndimage.interpolation import map_coordinates\n",
    "\n",
    "data_dir = '/home/user/Robotics/Data_sets/slip_detection/will_dataset/data_collection_001_122/data_collection_001/'\n",
    "out_dir = '/home/user/Robotics/Data_sets/slip_detection/manual_slip_detection/'\n",
    "SAVE_IMAGES= True\n",
    "sequence_length = 20\n",
    "image_height, image_width = 32, 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-massachusetts",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the data:\n",
    "files = glob.glob(data_dir + '/*')\n",
    "path_file = []\n",
    "index_to_save = 0\n",
    "\n",
    "xela_sensor1_data_x_final, xela_sensor1_data_y_final, xela_sensor1_data_z_final = [], [], []\n",
    "\n",
    "ee_positions_final = []\n",
    "ee_position_x_final = []\n",
    "ee_position_y_final = []\n",
    "ee_position_z_final = []\n",
    "ee_orientation_quat_x_final = []\n",
    "ee_orientation_quat_y_final = []\n",
    "ee_orientation_quat_z_final = []\n",
    "ee_orientation_quat_w_final = []\n",
    "ee_orientation_x_final = []\n",
    "ee_orientation_y_final = []\n",
    "ee_orientation_z_final = []\n",
    "\n",
    "exp_break_points = []\n",
    "exp_break_point = 0 \n",
    "\n",
    "for experiment_number in tqdm(range(len(files))):\n",
    "    robot_state  = np.asarray(pd.read_csv(files[experiment_number] + '/robot_state.csv', header=None))\n",
    "    proximity    = np.asarray(pd.read_csv(files[experiment_number] + '/proximity.csv', header=None))\n",
    "    xela_sensor1 = np.asarray(pd.read_csv(files[experiment_number] + '/xela_sensor1.csv', header=None))\n",
    "    xela_sensor2 = np.asarray(pd.read_csv(files[experiment_number] + '/xela_sensor2.csv', header=None))\n",
    "    meta_data = np.asarray(pd.read_csv(files[experiment_number] + '/meta_data.csv', header=None))\n",
    "\n",
    "    ee_positions = []\n",
    "    ee_position_x, ee_position_y, ee_position_z = [], [], []\n",
    "    ee_orientation_x, ee_orientation_y, ee_orientation_z = [], [], []\n",
    "    ee_orientation_quat_x, ee_orientation_quat_y, ee_orientation_quat_z, ee_orientation_quat_w = [], [], [], []\n",
    "\n",
    "    xela_sensor1_data_x, xela_sensor1_data_y, xela_sensor1_data_z = [], [], []\n",
    "    xela_sensor2_data_x, xela_sensor2_data_y, xela_sensor2_data_z = [], [], []\n",
    "    xela_sensor1_data_x_mean, xela_sensor1_data_y_mean, xela_sensor1_data_z_mean = [], [], []\n",
    "    xela_sensor2_data_x_mean, xela_sensor2_data_y_mean, xela_sensor2_data_z_mean = [], [], []\n",
    "    \n",
    "    ####################################### Robot Data ###########################################\n",
    "    for state in robot_state[1:]:\n",
    "        ee_positions.append([float(item) for item in robot_state[1][-7:-4]])\n",
    "        ee_position_x.append(state[-7])\n",
    "        ee_position_y.append(state[-6])\n",
    "        ee_position_z.append(state[-5])\n",
    "        # quat\n",
    "        ee_orientation_quat_x.append(state[-4])\n",
    "        ee_orientation_quat_y.append(state[-3])\n",
    "        ee_orientation_quat_z.append(state[-2])\n",
    "        ee_orientation_quat_w.append(state[-1])\n",
    "        # euler\n",
    "        ee_orientation = R.from_quat([state[-4], state[-3], state[-2], state[-1]]).as_euler('zyx', degrees=True)\n",
    "        ee_orientation_x.append(ee_orientation[0])\n",
    "        ee_orientation_y.append(ee_orientation[1])\n",
    "        ee_orientation_z.append(ee_orientation[2])\n",
    "        exp_break_point += 1\n",
    "\n",
    "    ####################################### Xela Data ###########################################\n",
    "    for sample1, sample2 in zip(xela_sensor1[1:], xela_sensor2[1:]):\n",
    "        sample1_data_x, sample1_data_y, sample1_data_z = [], [], []\n",
    "        sample2_data_x, sample2_data_y, sample2_data_z = [], [], []\n",
    "        for i in range(0, len(xela_sensor1[0]), 3):\n",
    "            sample1_data_x.append(float(sample1[i]))\n",
    "            sample1_data_y.append(float(sample1[i+1]))\n",
    "            sample1_data_z.append(float(sample1[i+2]))\n",
    "        xela_sensor1_data_x.append(sample1_data_x)\n",
    "        xela_sensor1_data_y.append(sample1_data_y)\n",
    "        xela_sensor1_data_z.append(sample1_data_z)\n",
    "\n",
    "    # mean starting values:\n",
    "    xela_sensor1_average_starting_value_x = int(sum(xela_sensor1_data_x[0]) / len(xela_sensor1_data_x[0]))\n",
    "    xela_sensor1_average_starting_value_y = int(sum(xela_sensor1_data_y[0]) / len(xela_sensor1_data_y[0]))\n",
    "    xela_sensor1_average_starting_value_z = int(sum(xela_sensor1_data_z[0]) / len(xela_sensor1_data_z[0]))\n",
    "    xela_sensor1_offset_x = [xela_sensor1_average_starting_value_x - tactile_starting_value for tactile_starting_value in xela_sensor1_data_x[0]]\n",
    "    xela_sensor1_offset_y = [xela_sensor1_average_starting_value_y - tactile_starting_value for tactile_starting_value in xela_sensor1_data_y[0]]\n",
    "    xela_sensor1_offset_z = [xela_sensor1_average_starting_value_z - tactile_starting_value for tactile_starting_value in xela_sensor1_data_z[0]]\n",
    "    \n",
    "    for time_step in range(len(xela_sensor1_data_x)):\n",
    "        xela_sensor1_sample_x_test = [offset+real_value for offset, real_value in zip(xela_sensor1_offset_x, xela_sensor1_data_x[time_step])]\n",
    "        xela_sensor1_sample_y_test = [offset+real_value for offset, real_value in zip(xela_sensor1_offset_y, xela_sensor1_data_y[time_step])]\n",
    "        xela_sensor1_sample_z_test = [offset+real_value for offset, real_value in zip(xela_sensor1_offset_z, xela_sensor1_data_z[time_step])]\n",
    "        for i in range(np.asarray(xela_sensor1_data_x).shape[1]):\n",
    "            xela_sensor1_data_x[time_step][i] = xela_sensor1_sample_x_test[i]\n",
    "            xela_sensor1_data_y[time_step][i] = xela_sensor1_sample_y_test[i] \n",
    "            xela_sensor1_data_z[time_step][i] = xela_sensor1_sample_z_test[i]\n",
    "    \n",
    "    xela_sensor1_data_x_final += xela_sensor1_data_x\n",
    "    xela_sensor1_data_y_final += xela_sensor1_data_y\n",
    "    xela_sensor1_data_z_final += xela_sensor1_data_z\n",
    "\n",
    "    ee_positions_final += ee_positions\n",
    "    ee_position_x_final += ee_position_x\n",
    "    ee_position_y_final += ee_position_y\n",
    "    ee_position_z_final += ee_position_z\n",
    "    ee_orientation_quat_x_final += ee_orientation_quat_x\n",
    "    ee_orientation_quat_y_final += ee_orientation_quat_y\n",
    "    ee_orientation_quat_z_final += ee_orientation_quat_z\n",
    "    ee_orientation_quat_w_final += ee_orientation_quat_w\n",
    "    ee_orientation_x_final += ee_orientation_x\n",
    "    ee_orientation_y_final += ee_orientation_y\n",
    "    ee_orientation_z_final += ee_orientation_z\n",
    "\n",
    "    exp_break_points.append(exp_break_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generic-publication",
   "metadata": {},
   "source": [
    "**Here we want to make any pre-processing changes to the data and keep hold of the scalars for upscalling later on.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disabled-refund",
   "metadata": {},
   "outputs": [],
   "source": [
    "xela_sensor1_data_x_final = np.asarray(xela_sensor1_data_x_final)\n",
    "xela_sensor1_data_y_final = np.asarray(xela_sensor1_data_y_final)\n",
    "xela_sensor1_data_z_final = np.asarray(xela_sensor1_data_z_final)\n",
    "ee_orientation_quat_x_final = np.asarray(ee_orientation_quat_x_final).reshape(-1, 1)\n",
    "ee_orientation_quat_y_final = np.asarray(ee_orientation_quat_y_final).reshape(-1, 1)\n",
    "ee_orientation_quat_z_final = np.asarray(ee_orientation_quat_z_final).reshape(-1, 1)\n",
    "ee_orientation_quat_w_final = np.asarray(ee_orientation_quat_w_final).reshape(-1, 1)\n",
    "\n",
    "scaler_x = preprocessing.StandardScaler().fit(xela_sensor1_data_x_final)\n",
    "scaler_y = preprocessing.StandardScaler().fit(xela_sensor1_data_y_final)\n",
    "scaler_z = preprocessing.StandardScaler().fit(xela_sensor1_data_z_final)\n",
    "xela_sensor1_data_x_scaled = scaler_x.transform(xela_sensor1_data_x_final)\n",
    "xela_sensor1_data_y_scaled = scaler_y.transform(xela_sensor1_data_y_final)\n",
    "xela_sensor1_data_z_scaled = scaler_z.transform(xela_sensor1_data_z_final)\n",
    "\n",
    "min_max_scaler_x = preprocessing.MinMaxScaler(feature_range=(-1, 1)).fit(xela_sensor1_data_x_scaled)\n",
    "xela_sensor1_data_x_scaled_minmax = min_max_scaler_x.transform(xela_sensor1_data_x_scaled)\n",
    "min_max_scaler_y = preprocessing.MinMaxScaler(feature_range=(-1, 1)).fit(xela_sensor1_data_y_scaled)\n",
    "xela_sensor1_data_y_scaled_minmax = min_max_scaler_y.transform(xela_sensor1_data_y_scaled)\n",
    "min_max_scaler_z = preprocessing.MinMaxScaler(feature_range=(-1, 1)).fit(xela_sensor1_data_z_scaled)\n",
    "xela_sensor1_data_z_scaled_minmax = min_max_scaler_z.transform(xela_sensor1_data_z_scaled)\n",
    "\n",
    "xela_sensor1_data = np.concatenate((xela_sensor1_data_x_final, xela_sensor1_data_y_final, xela_sensor1_data_z_final), axis=1)\n",
    "scaler_full = preprocessing.StandardScaler().fit(xela_sensor1_data)\n",
    "xela_sensor1_data_scaled = scaler_full.transform(xela_sensor1_data)\n",
    "min_max_scaler_full_data = preprocessing.MinMaxScaler(feature_range=(-1, 1)).fit(xela_sensor1_data_scaled)\n",
    "xela_sensor1_data_scaled_minmax = min_max_scaler_full_data.transform(xela_sensor1_data_scaled)\n",
    "\n",
    "scaler_quat_x = preprocessing.StandardScaler().fit(ee_orientation_quat_x_final)\n",
    "ee_orientation_quat_x_final_scaled = scaler_quat_x.transform(ee_orientation_quat_x_final)\n",
    "\n",
    "scaler_quat_y = preprocessing.StandardScaler().fit(ee_orientation_quat_y_final)\n",
    "ee_orientation_quat_y_final_scaled = scaler_quat_y.transform(ee_orientation_quat_y_final)\n",
    "\n",
    "scaler_quat_z = preprocessing.StandardScaler().fit(ee_orientation_quat_z_final)\n",
    "ee_orientation_quat_z_final_scaled = scaler_quat_z.transform(ee_orientation_quat_z_final)\n",
    "\n",
    "scaler_quat_w = preprocessing.StandardScaler().fit(ee_orientation_quat_w_final)\n",
    "ee_orientation_quat_w_final_scaled = scaler_quat_w.transform(ee_orientation_quat_w_final)\n",
    "\n",
    "min_max_scaler_quat_x = preprocessing.MinMaxScaler(feature_range=(-1, 1)).fit(ee_orientation_quat_x_final_scaled)\n",
    "ee_orientation_quat_x_final_scaled_minmax = min_max_scaler_quat_x.transform(ee_orientation_quat_x_final_scaled)\n",
    "min_max_scaler_quat_y = preprocessing.MinMaxScaler(feature_range=(-1, 1)).fit(ee_orientation_quat_y_final_scaled)\n",
    "ee_orientation_quat_y_final_scaled_minmax = min_max_scaler_quat_y.transform(ee_orientation_quat_y_final_scaled)\n",
    "min_max_scaler_quat_z = preprocessing.MinMaxScaler(feature_range=(-1, 1)).fit(ee_orientation_quat_z_final_scaled)\n",
    "ee_orientation_quat_z_final_scaled_minmax = min_max_scaler_quat_z.transform(ee_orientation_quat_z_final_scaled)\n",
    "min_max_scaler_quat_w = preprocessing.MinMaxScaler(feature_range=(-1, 1)).fit(ee_orientation_quat_w_final_scaled)\n",
    "ee_orientation_quat_w_final_scaled_minmax = min_max_scaler_quat_w.transform(ee_orientation_quat_w_final_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intimate-commerce",
   "metadata": {},
   "source": [
    "**Here we do dimensionality reduction**\n",
    "\n",
    "Plus any analysis you want to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dirty-wallpaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=10)\n",
    "pca.fit(xela_sensor1_data_scaled)\n",
    "cov = pca.get_covariance()\n",
    "plt.matshow(cov)\n",
    "cb = plt.colorbar()\n",
    "cb.ax.tick_params(labelsize=12)\n",
    "plt.title(\"full data covariance matrix\")\n",
    "plt.show()\n",
    "\n",
    "pca = PCA(n_components=10)\n",
    "principalComponents = pca.fit_transform(xela_sensor1_data_scaled)\n",
    "pca.fit(xela_sensor1_data_scaled)\n",
    "score = pca.score(xela_sensor1_data_scaled)\n",
    "cov = pca.get_covariance()\n",
    "xela_sensor1_principle_components = pca.transform(xela_sensor1_data_scaled)\n",
    "inverse_transformed_data = pca.inverse_transform(xela_sensor1_principle_components)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "feature = [i for i in range(len(pca.explained_variance_ratio_))]\n",
    "variance_explained = pca.explained_variance_ratio_\n",
    "ax.bar(feature,variance_explained)\n",
    "plt.title(\"variance explained ratio\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sorted-independence",
   "metadata": {},
   "source": [
    "**Run the dimensianlly reduced vector through the model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-rates",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import csv\n",
    "import tqdm\n",
    "import copy\n",
    "import click\n",
    "import logging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from string import digits\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "\n",
    "seed = 42\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "learning_rate = 1e-3\n",
    "context_frames = 10\n",
    "sequence_length = 20\n",
    "lookback = sequence_length\n",
    "\n",
    "valid_train_split = 0.8  # precentage of train data from total\n",
    "test_train_split = 0.9  # precentage of train data from total\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")#  use gpu if available\n",
    "################################# CHANGE THIS!!!!  #################################\n",
    "model_path = \"/home/user/Robotics/slip_detection_model/slip_detection_model/manual_data_models/models/simple_model_001/\"\n",
    "################################# CHANGE THIS!!!!  #################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dress-physiology",
   "metadata": {},
   "source": [
    "**Change the \"xela_sensor1_principle_components\" to your data -- line 12** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "taken-monthly",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale between 0 and 1:\n",
    "ee_position_x_final = np.array(ee_position_x_final) \n",
    "ee_position_y_final = np.array(ee_position_y_final) \n",
    "ee_position_z_final = np.array(ee_position_z_final) \n",
    "ee_orientation_quat_x_final = np.array(ee_orientation_quat_x_final) \n",
    "ee_orientation_quat_y_final = np.array(ee_orientation_quat_y_final) \n",
    "ee_orientation_quat_z_final = np.array(ee_orientation_quat_z_final) \n",
    "ee_orientation_quat_w_final = np.array(ee_orientation_quat_w_final) \n",
    "ee_orientation_x_final = np.array(ee_orientation_x_final) \n",
    "ee_orientation_y_final = np.array(ee_orientation_y_final) \n",
    "ee_orientation_z_final = np.array(ee_orientation_z_final) \n",
    "xela_sensor1_principle_components = np.array(xela_sensor1_principle_components) \n",
    "\n",
    "min_max_scaler_ee_position_x_final = preprocessing.MinMaxScaler(feature_range=(-1, 1)).fit(ee_position_x_final.reshape(-1, 1))\n",
    "ee_position_x_final_scaled = min_max_scaler_ee_position_x_final.transform(ee_position_x_final.reshape(-1, 1))\n",
    "\n",
    "min_max_scaler_ee_position_y_final = preprocessing.MinMaxScaler(feature_range=(-1, 1)).fit(ee_position_y_final.reshape(-1, 1))\n",
    "ee_position_y_final_scaled = min_max_scaler_ee_position_y_final.transform(ee_position_y_final.reshape(-1, 1))\n",
    "\n",
    "min_max_scaler_ee_position_z_final = preprocessing.MinMaxScaler(feature_range=(-1, 1)).fit(ee_position_z_final.reshape(-1, 1))\n",
    "ee_position_z_final_scaled = min_max_scaler_ee_position_z_final.transform(ee_position_z_final.reshape(-1, 1))\n",
    "\n",
    "min_max_scaler_ee_orientation_quat_x_final = preprocessing.MinMaxScaler(feature_range=(-1, 1)).fit(ee_orientation_quat_x_final)\n",
    "ee_orientation_quat_x_final_scaled = min_max_scaler_ee_orientation_quat_x_final.transform(ee_orientation_quat_x_final)\n",
    "\n",
    "min_max_scaler_ee_orientation_quat_y_final = preprocessing.MinMaxScaler(feature_range=(-1, 1)).fit(ee_orientation_quat_y_final)\n",
    "ee_orientation_quat_y_final_scaled = min_max_scaler_ee_orientation_quat_y_final.transform(ee_orientation_quat_y_final)\n",
    "\n",
    "min_max_scaler_ee_orientation_quat_z_final = preprocessing.MinMaxScaler(feature_range=(-1, 1)).fit(ee_orientation_quat_z_final)\n",
    "ee_orientation_quat_z_final_scaled = min_max_scaler_ee_orientation_quat_z_final.transform(ee_orientation_quat_z_final)\n",
    "\n",
    "min_max_scaler_ee_orientation_quat_w_final = preprocessing.MinMaxScaler(feature_range=(-1, 1)).fit(ee_orientation_quat_w_final)\n",
    "ee_orientation_quat_w_final_scaled = min_max_scaler_ee_orientation_quat_w_final.transform(ee_orientation_quat_w_final)\n",
    "\n",
    "min_max_scaler_ee_orientation_x_final = preprocessing.MinMaxScaler(feature_range=(-1, 1)).fit(ee_orientation_x_final.reshape(-1, 1))\n",
    "ee_orientation_x_final_scaled = min_max_scaler_ee_orientation_x_final.transform(ee_orientation_x_final.reshape(-1, 1))\n",
    "\n",
    "min_max_scaler_ee_orientation_y_final = preprocessing.MinMaxScaler(feature_range=(-1, 1)).fit(ee_orientation_y_final.reshape(-1, 1))\n",
    "ee_orientation_y_final_scaled = min_max_scaler_ee_orientation_y_final.transform(ee_orientation_y_final.reshape(-1, 1))\n",
    "\n",
    "min_max_scaler_ee_orientation_z_final = preprocessing.MinMaxScaler(feature_range=(-1, 1)).fit(ee_orientation_z_final.reshape(-1, 1))\n",
    "ee_orientation_z_final_scaled = min_max_scaler_ee_orientation_z_final.transform(ee_orientation_z_final.reshape(-1, 1))\n",
    "\n",
    "min_max_scaler_xela_sensor1_principle_components = preprocessing.MinMaxScaler(feature_range=(-1, 1)).fit(xela_sensor1_principle_components)\n",
    "xela_sensor1_principle_components_scaled = min_max_scaler_xela_sensor1_principle_components.transform(xela_sensor1_principle_components)\n",
    "\n",
    "# Convert data back into split experiments to create the sequences:\n",
    "ee_position_x_final_split = np.asarray(np.split(ee_position_x_final_scaled, exp_break_points)[0:-1])\n",
    "ee_position_y_final_split = np.asarray(np.split(ee_position_y_final_scaled, exp_break_points)[0:-1])\n",
    "ee_position_z_final_split = np.asarray(np.split(ee_position_z_final_scaled, exp_break_points)[0:-1])\n",
    "ee_orientation_quat_x_final_split = np.asarray(np.split(ee_orientation_quat_x_final_scaled, exp_break_points)[0:-1])\n",
    "ee_orientation_quat_y_final_split = np.asarray(np.split(ee_orientation_quat_y_final_scaled, exp_break_points)[0:-1])\n",
    "ee_orientation_quat_z_final_split = np.asarray(np.split(ee_orientation_quat_z_final_scaled, exp_break_points)[0:-1])\n",
    "ee_orientation_quat_w_final_split = np.asarray(np.split(ee_orientation_quat_w_final_scaled, exp_break_points)[0:-1])\n",
    "ee_orientation_x_final_split = np.asarray(np.split(ee_orientation_x_final_scaled, exp_break_points)[0:-1])\n",
    "ee_orientation_y_final_split = np.asarray(np.split(ee_orientation_y_final_scaled, exp_break_points)[0:-1])\n",
    "ee_orientation_z_final_split = np.asarray(np.split(ee_orientation_z_final_scaled, exp_break_points)[0:-1])\n",
    "xela_sensor1_principle_components_split = np.asarray(np.split(xela_sensor1_principle_components_scaled, exp_break_points)[0:-1])\n",
    "\n",
    "# Shuffle data:\n",
    "p = np.random.permutation(len(ee_position_x_final_split))\n",
    "p = np.delete(p, np.where(p==106))  # make 106 always in the test set (for comparing graphs)\n",
    "p = np.append(p, [106])\n",
    "print(\"shuffle order: \", p)\n",
    "ee_position_x_final_split = ee_position_x_final_split[p]\n",
    "ee_position_y_final_split = ee_position_y_final_split[p]\n",
    "ee_position_z_final_split = ee_position_z_final_split[p]\n",
    "ee_orientation_quat_x_final_split = ee_orientation_quat_x_final_split[p]\n",
    "ee_orientation_quat_y_final_split = ee_orientation_quat_y_final_split[p]\n",
    "ee_orientation_quat_z_final_split = ee_orientation_quat_z_final_split[p]\n",
    "ee_orientation_quat_w_final_split = ee_orientation_quat_w_final_split[p]\n",
    "ee_orientation_x_final_split = ee_orientation_x_final_split[p]\n",
    "ee_orientation_y_final_split = ee_orientation_y_final_split[p]\n",
    "ee_orientation_z_final_split = ee_orientation_z_final_split[p]\n",
    "xela_sensor1_principle_components_split = xela_sensor1_principle_components_split[p]\n",
    "\n",
    "# convert to sequences:\n",
    "robot_data_euler_sequence, robot_data_quat_sequence, xela_1_sequence_data, experiment_data_sequence, time_step_data_sequence = [], [], [], [], []\n",
    "for experiment in range(len(ee_position_x_final_split)):\n",
    "    for sample in range(0, len(ee_position_x_final_split[experiment]) - sequence_length):\n",
    "        robot_data_euler_sample, robot_data_quat_sample, xela_1_sequ_sample, experiment_data_sample, time_step_data_sample = [], [], [], [], []\n",
    "        for t in range(0, sequence_length):\n",
    "            robot_data_euler_sample.append([ee_position_x_final_split[experiment][sample+t], ee_position_y_final_split[experiment][sample+t], ee_position_z_final_split[experiment][sample+t], ee_orientation_x_final_split[experiment][sample+t], ee_orientation_y_final_split[experiment][sample+t], ee_orientation_z_final_split[experiment][sample+t]])\n",
    "            robot_data_quat_sample.append([ee_position_x_final_split[experiment][sample+t], ee_position_y_final_split[experiment][sample+t], ee_position_z_final_split[experiment][sample+t], ee_orientation_quat_x_final_split[experiment][sample+t][0], ee_orientation_quat_y_final_split[experiment][sample+t][0], ee_orientation_quat_z_final_split[experiment][sample+t][0], ee_orientation_quat_w_final_split[experiment][sample+t][0]])\n",
    "            xela_1_sequ_sample.append(xela_sensor1_principle_components_split[experiment][sample+t])\n",
    "            experiment_data_sample.append(experiment)\n",
    "            time_step_data_sample.append(sample+t)\n",
    "        robot_data_euler_sequence.append(robot_data_euler_sample)\n",
    "        robot_data_quat_sequence.append(robot_data_quat_sample)\n",
    "        xela_1_sequence_data.append(xela_1_sequ_sample)\n",
    "        experiment_data_sequence.append(experiment_data_sample)\n",
    "        time_step_data_sequence.append(time_step_data_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baking-advice",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchGenerator:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def load_full_data(self):\n",
    "        dataset_train = FullDataSet(type_=\"train\")\n",
    "        dataset_valid = FullDataSet(type_=\"valid\")\n",
    "        dataset_test = FullDataSet(type_=\"test\")\n",
    "        transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "        train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "        valid_loader = torch.utils.data.DataLoader(dataset_valid, batch_size=batch_size, shuffle=True)\n",
    "        test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size, shuffle=False)\n",
    "        return train_loader, valid_loader, test_loader\n",
    "\n",
    "\n",
    "class FullDataSet():\n",
    "    def __init__(self, type_=\"train\"):\n",
    "        if type_ == \"train\":\n",
    "            self.samples = [0, int(len(robot_data_euler_sequence)*test_train_split)]\n",
    "        elif type_ == \"valid\":\n",
    "            self.samples = [int(len(robot_data_euler_sequence)*(valid_train_split)), int(len(robot_data_euler_sequence)*test_train_split)]\n",
    "        elif type_ == \"test\":\n",
    "            self.samples = [int(len(robot_data_euler_sequence)*test_train_split), len(robot_data_euler_sequence)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.samples[1] - self.samples[0]\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "#         robot_euler = robot_data_euler_sequence[self.samples[0] + idx]\n",
    "        robot_quat = robot_data_quat_sequence[self.samples[0] + idx]\n",
    "        xela1 = xela_1_sequence_data[self.samples[0] + idx]\n",
    "        experiment = experiment_data_sequence[self.samples[0] + idx]\n",
    "        time_step  = time_step_data_sequence[self.samples[0] + idx]\n",
    "        return([np.array(robot_quat).astype(np.float32),\n",
    "                 np.array(xela1).astype(np.float32),\n",
    "                 np.array(experiment),\n",
    "                 np.array(time_step)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composed-present",
   "metadata": {},
   "source": [
    "**Adjust the lstm1 and hidden1 size to fit the data you have.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complex-junction",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FullModel, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(48, 48).to(device)  # tactile\n",
    "        self.lstm2 = nn.LSTM(7, 7).to(device)  # pos_vel\n",
    "        self.fc1 = nn.Linear(48 + 49 + 48, 48).to(device)  # tactile + pos_vel\n",
    "\n",
    "    def forward(self, tactiles, actions):\n",
    "        state = actions[0]\n",
    "        state.to(device)\n",
    "        batch_size__ = tactiles.shape[1]\n",
    "        outputs = []\n",
    "        hidden1 = (torch.rand(1,batch_size__,48).to(device), torch.rand(1,batch_size__,48).to(device))\n",
    "        hidden2 = (torch.rand(1,batch_size__,7).to(device), torch.rand(1,batch_size__,7).to(device))\n",
    "        for index, (sample_tactile, sample_action) in enumerate(zip(tactiles.squeeze(), actions.squeeze())):\n",
    "            sample_tactile.to(device)\n",
    "            sample_action.to(device)\n",
    "            # 2. Run through lstm:\n",
    "            if index > context_frames-1:\n",
    "                out1, hidden1 = self.lstm1(out3.to(device), hidden1)\n",
    "                out2, hidden2 = self.lstm2(sample_action.unsqueeze(0), hidden2)\n",
    "                tactile_tiled = torch.cat((out2.squeeze(), out2.squeeze(), out2.squeeze(), out2.squeeze(), out2.squeeze(), out2.squeeze(), out2.squeeze()), 1)\n",
    "                robot_and_tactile = torch.cat((tactile_tiled.squeeze(), out1.squeeze(), out3.squeeze()), 1)\n",
    "                out3 = self.fc1(robot_and_tactile.unsqueeze(0))\n",
    "                outputs.append(out3.squeeze())\n",
    "            else:\n",
    "                out1, hidden1 = self.lstm1(sample_tactile.unsqueeze(0), hidden1)\n",
    "                out2, hidden2 = self.lstm2(sample_action.unsqueeze(0), hidden2)\n",
    "                tactile_tiled = torch.cat((out2.squeeze(), out2.squeeze(), out2.squeeze(), out2.squeeze(), out2.squeeze(), out2.squeeze(), out2.squeeze()), 1)\n",
    "                robot_and_tactile = torch.cat((tactile_tiled.squeeze(), out1.squeeze(), sample_tactile), 1)\n",
    "                out3 = self.fc1(robot_and_tactile.unsqueeze(0))\n",
    "\n",
    "        return torch.stack(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spanish-wages",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.train_full_loader, self.valid_full_loader, self.test_full_loader = BG.load_full_data()\n",
    "        self.full_model = FullModel()\n",
    "        self.criterion = nn.L1Loss()\n",
    "        self.optimizer = optim.Adam(self.full_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    def train_full_model(self):\n",
    "        plot_training_loss = []\n",
    "        plot_validation_loss = []\n",
    "        previous_val_mean_loss = 100.0\n",
    "        best_val_loss = 100.0\n",
    "        early_stop_clock = 0\n",
    "        progress_bar = tqdm.tqdm(range(0, epochs), total=(epochs*len(self.train_full_loader)))\n",
    "        mean_test = 0\n",
    "        for epoch in progress_bar:\n",
    "            loss = 0\n",
    "            losses = 0.0\n",
    "            for index, batch_features in enumerate(self.train_full_loader):\n",
    "                action = batch_features[0].permute(1,0,2).to(device)\n",
    "                tactile = batch_features[1].permute(1,0,2).to(device)\n",
    "\n",
    "                tactile_predictions = self.full_model.forward(tactiles=tactile, actions=action) # Step 3. Run our forward pass.\n",
    "                self.optimizer.zero_grad()\n",
    "                loss = self.criterion(tactile_predictions.to(device), tactile[context_frames:])\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                losses += loss.item()\n",
    "                if index:\n",
    "                    mean = losses / index\n",
    "                else:\n",
    "                    mean = 0\n",
    "                progress_bar.set_description(\"epoch: {}, \".format(epoch) + \"loss: {:.4f}, \".format(float(loss.item())) + \"mean loss: {:.4f}, \".format(mean))\n",
    "                progress_bar.update()\n",
    "            plot_training_loss.append(mean)\n",
    "\n",
    "            val_losses = 0.0\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for index__, batch_features in enumerate(self.valid_full_loader):\n",
    "                    action = batch_features[0].permute(1,0,2).to(device)\n",
    "                    tactile = batch_features[1].permute(1,0,2).to(device)\n",
    "\n",
    "                    tactile_predictions = self.full_model.forward(tactiles=tactile, actions=action)  # Step 3. Run our forward pass.\n",
    "                    self.optimizer.zero_grad()\n",
    "                    val_loss = self.criterion(tactile_predictions.to(device), tactile[context_frames:])\n",
    "                    val_losses += val_loss.item()\n",
    "\n",
    "            print(\"Validation mean loss: {:.4f}, \".format(val_losses / index__))\n",
    "            plot_validation_loss.append(val_losses / index__)\n",
    "            if previous_val_mean_loss < val_losses / index__:\n",
    "                early_stop_clock +=1\n",
    "                previous_val_mean_loss = val_losses / index__ \n",
    "                if early_stop_clock == 2:\n",
    "                    print(\"Early stopping\")\n",
    "                    break\n",
    "            else:\n",
    "                if best_val_loss > val_losses / index__:\n",
    "                    self.strongest_model = copy.deepcopy(self.full_model)\n",
    "                early_stop_clock = 0\n",
    "                previous_val_mean_loss = val_losses / index__ \n",
    "        plt.plot(plot_training_loss, c=\"r\", label=\"train loss MAE\")\n",
    "        plt.plot(plot_validation_loss, c='b', label=\"val loss MAE\")\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.show()\n",
    "        np.save(model_path + 'pca_model_training_loss', np.asarray(plot_training_loss))\n",
    "        np.save(model_path + 'pca_model_validation_loss', np.asarray(plot_validation_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "european-houston",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create data generator, train model and save model.\n",
    "BG = BatchGenerator()\n",
    "print(\"done\")\n",
    "MT = ModelTrainer()\n",
    "MT.train_full_model()\n",
    "print(\"finished training\")\n",
    "torch.save(MT.strongest_model, model_path + \"pca_full_model\")\n",
    "model = torch.load(model_path + \"pca_full_model\")\n",
    "model.eval()\n",
    "print(\"saved the model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynamic-indian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model on the full test sample:\n",
    "# model = MT.strongest_model\n",
    "\n",
    "criterion1 = nn.L1Loss()\n",
    "criterion2 = nn.MSELoss()\n",
    "\n",
    "test_lossesMAE_t1 = 0.0\n",
    "test_lossesMSE_t1 = 0.0\n",
    "test_lossesMAE_t5 = 0.0\n",
    "test_lossesMSE_t5 = 0.0\n",
    "test_lossesMAE_t10 = 0.0\n",
    "test_lossesMSE_t10 = 0.0\n",
    "\n",
    "tactile_predictions = []\n",
    "tactile_groundtruth = []\n",
    "experiment_time_steps = []\n",
    "test_lossesMAE = 0.0\n",
    "test_lossesMSE = 0.0\n",
    "with torch.no_grad():\n",
    "    for index__, batch_features in enumerate(MT.test_full_loader):\n",
    "        # 2. Reshape data and send to device:\n",
    "        action = batch_features[0].permute(1,0,2).to(device)\n",
    "        tactile = batch_features[1].permute(1,0,2).to(device)\n",
    "\n",
    "        tp = model.forward(tactiles=tactile, actions=action)\n",
    "        experiment_time_steps.append([batch_features[2], batch_features[3]])\n",
    "        \n",
    "        ############ RESCALE THE DATA HERE ################\n",
    "        ############ RESCALE THE DATA HERE ################\n",
    "        ############ RESCALE THE DATA HERE ################\n",
    "        ############ RESCALE THE DATA HERE ################\n",
    "        tp = backscale(tp)\n",
    "        ############ RESCALE THE DATA HERE ################\n",
    "        ############ RESCALE THE DATA HERE ################\n",
    "        ############ RESCALE THE DATA HERE ################\n",
    "        ############ RESCALE THE DATA HERE ################\n",
    "\n",
    "        tactile_predictions.append(tp)  # Step 3. Run our forward pass.\n",
    "        tactile_groundtruth.append(tactile[context_frames:])\n",
    "        # calculate losses for specific timesteps\n",
    "        test_lossMAE_t1 = criterion1(tp[0,:,:].to(device), tactile[context_frames:][0,:,:])\n",
    "        test_lossesMAE_t1 += test_lossMAE_t1.item()\n",
    "        test_lossMSE_t1 = criterion2(tp[0,:,:].to(device), tactile[context_frames:][0,:,:])\n",
    "        test_lossesMSE_t1 += test_lossMSE_t1.item()\n",
    "        test_lossMAE_t5 = criterion1(tp[4,:,:].to(device), tactile[context_frames:][4,:,:])\n",
    "        test_lossesMAE_t5 += test_lossMAE_t5.item()\n",
    "        test_lossMSE_t5 = criterion2(tp[4,:,:].to(device), tactile[context_frames:][4,:,:])\n",
    "        test_lossesMSE_t5 += test_lossMSE_t5.item()\n",
    "        test_lossMAE_t10 = criterion1(tp[9,:,:].to(device), tactile[context_frames:][9,:,:])\n",
    "        test_lossesMAE_t10 += test_lossMAE_t10.item()\n",
    "        test_lossMSE_t10 = criterion2(tp[9,:,:].to(device), tactile[context_frames:][9,:,:])\n",
    "        test_lossesMSE_t10 += test_lossMSE_t10.item()\n",
    "\n",
    "performance_data = []\n",
    "performance_data.append([\"test loss MAE(L1): \", (test_lossesMAE / index__)])\n",
    "performance_data.append([\"test loss MSE: \", (test_lossesMSE / index__)])\n",
    "performance_data.append([\"test loss MAE(L1) timestep 1: \", (test_lossesMAE_t1 / index__)])\n",
    "performance_data.append([\"test loss MSE timestep 1: \", (test_lossesMSE_t1 / index__)])\n",
    "performance_data.append([\"test loss MAE(L1) timestep 5: \", (test_lossesMAE_t5 / index__)])\n",
    "performance_data.append([\"test loss MSE timestep 5: \", (test_lossesMSE_t5 / index__)])\n",
    "performance_data.append([\"test loss MAE(L1) timestep 9: \", (test_lossesMAE_t10 / index__)])\n",
    "performance_data.append([\"test loss MSE timestep 9: \", (test_lossesMSE_t10 / index__)])\n",
    "[print(i) for i in performance_data]\n",
    "\n",
    "np.save(model_path + 'pca_performance_data', np.asarray(performance_data))\n",
    "\n",
    "# calculate tactile values for full sample:\n",
    "time_step_to_test_t1 = 0    # [batch_set, prediction frames(t1->tx)(6), batch_size, features(48)]\n",
    "time_step_to_test_t9 = 5\n",
    "predicted_data_t1 = []\n",
    "predicted_data_t9 = []\n",
    "groundtruth_data = []\n",
    "for index, batch_set in enumerate(tactile_predictions):\n",
    "    for batch in range(0, len(batch_set[0])):\n",
    "        prediction_values = batch_set[time_step_to_test_t1][batch]\n",
    "        predicted_data_t1.append(prediction_values)\n",
    "        prediction_values = batch_set[time_step_to_test_t9][batch]\n",
    "        predicted_data_t9.append(prediction_values)\n",
    "        gt_values = tactile_groundtruth[index][time_step_to_test_t1][batch]\n",
    "        groundtruth_data.append(gt_values)  \n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfied-depth",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import (AutoMinorLocator, MultipleLocator)\n",
    "# calculate tactile values for full sample:\n",
    "time_step_to_test_t1 = 0    # [batch_set, prediction frames(t1->tx)(6), batch_size, features(48)]\n",
    "time_step_to_test_t5 = 4\n",
    "time_step_to_test_t9 = 9\n",
    "predicted_data_t1 = []\n",
    "predicted_data_t5 = []\n",
    "predicted_data_t9 = []\n",
    "groundtruth_data = []\n",
    "experiment_to_test = 106\n",
    "for index, batch_set in enumerate(tactile_predictions):\n",
    "    for batch in range(0, len(batch_set[0])):\n",
    "        experiment = experiment_time_steps[index][0][batch][0]\n",
    "        if experiment == experiment_to_test:\n",
    "            if not prediction: ## for use in single prediction plotting \n",
    "                single_prediction = batch_set\n",
    "                single_ground_truth = tactile_groundtruth[index]\n",
    "            prediction_values = batch_set[time_step_to_test_t1][batch]\n",
    "            predicted_data_t1.append(prediction_values)\n",
    "            prediction_values = batch_set[time_step_to_test_t5][batch]\n",
    "            predicted_data_t5.append(prediction_values)\n",
    "            prediction_values = batch_set[time_step_to_test_t9][batch]\n",
    "            predicted_data_t9.append(prediction_values)\n",
    "            gt_values = tactile_groundtruth[index][time_step_to_test_t1][batch]\n",
    "            groundtruth_data.append(gt_values)\n",
    "\n",
    "# plt.show()     \n",
    "# mse_loss = torch.nn.MSELoss()\n",
    "# mae_loss = torch.nn.L1Loss()\n",
    "\n",
    "# test data\n",
    "index = 0\n",
    "titles = [\"sheerx\", \"sheery\", \"normal\"]\n",
    "for j in range(3):\n",
    "    for i in range(16):\n",
    "        groundtruth_taxle = []\n",
    "        predicted_taxel = []\n",
    "        predicted_taxel_t1 = []\n",
    "        predicted_taxel_t5 = []\n",
    "        predicted_taxel_t9 = []\n",
    "        # good = 140, 145 (lifting up the )\n",
    "        for k in range(len(predicted_data_t1)):#310, 325):#len(predicted_data_t1)):  # add in length of context data\n",
    "            predicted_taxel_t1.append(predicted_data_t1[k][j+i].cpu().detach().numpy())\n",
    "            predicted_taxel_t5.append(predicted_data_t5[k][j+i].cpu().detach().numpy())\n",
    "            predicted_taxel_t9.append(predicted_data_t9[k][j+i].cpu().detach().numpy())\n",
    "            groundtruth_taxle.append(groundtruth_data[k][j+i].cpu().detach().numpy())\n",
    "\n",
    "        index += 1\n",
    "        fig, ax1 = plt.subplots()\n",
    "        ax1.set_xlabel('time step')\n",
    "        ax1.set_ylabel('tactile reading')\n",
    "        ax1.plot([None for i in range(0)] + [i for i in predicted_taxel_t1], alpha=0.5, c=\"b\", label=\"t1\")\n",
    "        ax1.plot([None for i in range(4)] + [i for i in predicted_taxel_t5], alpha=0.5, c=\"k\", label=\"t5\")\n",
    "        ax1.plot([None for i in range(9)] + [i for i in predicted_taxel_t9], alpha=0.5, c=\"g\", label=\"t10\")\n",
    "        ax1.plot(groundtruth_taxle, alpha=0.5, c=\"r\", label=\"gt\")\n",
    "        ax1.tick_params(axis='y')\n",
    "        ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "        ax2.set_ylabel('loss')  # we already handled the x-label with ax1\n",
    "        fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "        fig.subplots_adjust(top=0.90)\n",
    "        ax1.legend(loc=\"upper right\")\n",
    "        plt.title(\"Simple_LSTM tactile \" + str(index))\n",
    "        plt.savefig(model_path + '/' + str(experiment_to_test) + '/pca_model_sample_test_with_loss_' + str(index) + '.png', dpi=300)\n",
    "        plt.show()\n",
    "\n",
    "        fig, ax1 = plt.subplots()\n",
    "        ax1.set_xlabel('time step')\n",
    "        ax1.set_ylabel('tactile reading')\n",
    "        ax1.plot([None for i in range(0)] + [i for i in predicted_taxel_t1], alpha=0.5, c=\"b\", label=\"t1\")\n",
    "        ax1.plot(groundtruth_taxle, alpha=0.5, c=\"r\", label=\"gt\")\n",
    "        ax1.tick_params(axis='y')\n",
    "        ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "        ax2.set_ylabel('loss')  # we already handled the x-label with ax1\n",
    "        fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "        fig.subplots_adjust(top=0.90)\n",
    "        ax1.legend(loc=\"upper right\")\n",
    "        plt.title(\"Simple_LSTM tactile \" + str(index))\n",
    "        plt.savefig(model_path + '/' + str(experiment_to_test) + '/pca_model_T0sample_test_with_loss_' + str(index) + '.png', dpi=300)\n",
    "        plt.show()\n",
    "        \n",
    "        fig, ax1 = plt.subplots()\n",
    "        ax1.set_xlabel('time step')\n",
    "        ax1.set_ylabel('tactile reading')\n",
    "        ax1.plot([None for i in range(4)] + [i for i in predicted_taxel_t5], alpha=0.5, c=\"b\", label=\"t5\")\n",
    "        ax1.plot(groundtruth_taxle, alpha=0.5, c=\"r\", label=\"gt\")\n",
    "        ax1.tick_params(axis='y')\n",
    "        ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "        ax2.set_ylabel('loss')  # we already handled the x-label with ax1\n",
    "        fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "        fig.subplots_adjust(top=0.90)\n",
    "        ax1.legend(loc=\"upper right\")\n",
    "        plt.title(\"Simple_LSTM tactile \" + str(index))\n",
    "        plt.savefig(model_path + '/' + str(experiment_to_test) + '/pca_model_T5sample_test_with_loss_' + str(index) + '.png', dpi=300)\n",
    "        plt.show()\n",
    "            \n",
    "#         fig, ax1 = plt.subplots()\n",
    "#         ax1.set_xlabel('time step')\n",
    "#         ax1.set_ylabel('tactile reading')\n",
    "#         ax1.plot([None for i in range(9)] + [i for i in predicted_taxel_t9][140:180], alpha=0.5, c=\"b\", label=\"t10\")\n",
    "#         ax1.plot(groundtruth_taxle[140:180], alpha=0.5, c=\"r\", label=\"gt\")\n",
    "#         ax1.tick_params(axis='y')\n",
    "#         ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "#         ax2.set_ylabel('loss')  # we already handled the x-label with ax1\n",
    "# #         ax2.plot([i for i in range(len(groundtruth_data))], [mae_loss(predicted_data_t9[i].cpu().detach(), groundtruth_data[i].cpu().detach()) for i in range(len(groundtruth_data))], alpha=0.5)\n",
    "#         fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "#         fig.subplots_adjust(top=0.90)\n",
    "#         ax1.legend(loc=\"upper right\")\n",
    "#         ax1.xaxis.set_major_locator(MultipleLocator(10))\n",
    "#         ax1.xaxis.set_minor_locator(AutoMinorLocator(10))\n",
    "#         ax1.grid(which='minor')\n",
    "#         ax1.grid(which='major')\n",
    "#         plt.title(\"Simple_LSTM tactile \" + str(index))\n",
    "#         plt.savefig(model_path + '/' + str(experiment_to_test) + '/close_up_concat_model_T10sample_test_with_loss_' + str(index) + '.png', dpi=300)\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turned-password",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = single_prediction[:,0,:]\n",
    "ground_truth = single_ground_truth[:,0,:]\n",
    "\n",
    "print(prediction.shape)\n",
    "print(ground_truth.shape)\n",
    "\n",
    "for i in range(48):\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax1.set_xlabel('future time step')\n",
    "    ax1.set_ylabel('tactile reading')\n",
    "    ax1.plot(prediction[:,i].cpu().detach(), alpha=0.5, c=\"b\", label=\"single prediction\")\n",
    "    ax1.plot(ground_truth[:,i].cpu().detach(), alpha=0.5, c=\"r\", label=\"gt\")\n",
    "    ax1.tick_params(axis='y')\n",
    "    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "    fig.subplots_adjust(top=0.90)\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MultipleLocator(1))\n",
    "    ax1.grid(which='major')\n",
    "#     plt.savefig(model_path + '/' + str(experiment_to_test) + '/concat_model_single_prediction_sample_' + str(i) + '.png', dpi=300)\n",
    "    plt.title(\"Simple_LSTM for single prediction sequence (0-10), tactile \" + str(i))\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
