Tactile Prediction

Introduction:
Tactile sensation is an essential tool for inteligent interactions with a surrounding environment. In manipulation tasks in particular, we use tactile sensing to help inform and reinforce our understanding of an objects dynamics and physical properties beyond outputs from visual assesments. Grasping an objects who's weight was different than visual assesment suggested, or an objects center of mass was in an unexpected location or its material was slippier than expected are all examples where visual information must be reinforced with tactile data to produce reliable manipulation of an object. Visual assesment alone typically falls short due to  being physically remote and typically there is significant oclusion by the end effector at the points of contact \cite{tian2019manipulation}.

As manipulation tasks push from structured environments into more realistic real world states, the ability to utilise tactile information for manipulation control tasks becomes a more critical challange and still remains an open problem. The difficulty (and complete failure) of manipulation without tactile feedback was excellently shown in johansson et al. \cite{johansson2009coding} who anaesthetized peoples fingertips and had them attempt to perform a match striking task. It's importance within the robotics commintiy has also been stated in \cite{tian2019manipulation} \cite{yousef2011tactile} \cite{romeo2020methods} \cite{dahiya2009tactile}.

Romeo et al. \cite{romeo2020methods} state "control algorithms for force regulation or minimization and grasp stabilization" as a key open problems for tactile feedback. However, solving these problems has remained a fiendish problem to solve for the key reasons outlined in \cite{tian2019manipulation}, (i) tactile sensing technology and fabrication is limited to visual informateion of soft materials or sparse pointwise force measurements, well behind the resultion of a human skin. (ii) modelling contact forces between objects and fingertip are difficult to create. (iii) specifying a desired tactile signal for use in controlled manipulation is also a complex thing to define.

In this paper, we investigate the complexity of modelling the tactile dynamic behaviour of robot manipulation with in hand objects using tacile sensors with sparse pointwise force measurements. The contributions of this paper are as follows: We train a variety of deep neural networks and show that a multi-modal recurrant neural network can accurately predcit the future fingertip tactile readings of a robot grasping a slippery object while moving through a non linear trajectory. We show that this prediction model can be trained with entirely unlabeled data. Finally, we prove that accurate predictions can be made with 3D, unconfined trajectories of a 7DOF robotic manipulator with variance in pose, velocity and acceleration with a real object.  

We demonstrate and evaluate these contributions with a dataset of psudo random trajectories generated by human kinesthetic motions of a Franka Emika robot arm. This arm has two Xela uSkin (4x4 sensing elements called taxels) sensors attached to a two finger parallel type gripper. The force of the gripper is insufficient to keep the object in a stable location and so the motion of the robot creates slipping and eventually the object falls out of the robots grasp. The purpose of this dataset is to produce a rich set of sequences where the motion of the robot has direct effect on the sensation felt at the fingertips.

Related works:
Typically, research in the field of tactile sensor use for control is in the reactive application of slip detection, surveyed in depth in \cite{romeo2020methods}. However, the reserach into slip prediciton is lighter.

\cite{tian2019manipulation} proved that with accurate tactile predictions of a GelSight tactile sensor and a target tactile reading, model predictive control could be used to drive a robotic manipulator to the target tactile reading, in this case rolling a ball on the end of a fingertip to a desired location on a table. For prediction they used the convolutional dynamic neural advection model presented in \cite{finn2016unsupervised} which masks objects in an image and applies convolution to them to create a future image frame. 

The GelSight sensor used in \cite{tian2019manipulation} produces high-dimensional observations of the sensed object, which provide a strong "view" of the object, thus allowing the mechanism behind the CDNA model to work. Due to the difference in tactile sensor we expect that the low resolution uSkin tactile sensor reading cannot be applied to the same model. Furthermore, the considered tasks in this paper do not include grasping an object and only move the objects in 2D plane with a CNC machine which has the GelSight sensor mounted.

\cite{zhou2020learning} converted the xela uSkin tactile sensor readings to a visial representation that could be applied to the CDNA architecture that has proven success. The representation converts sheer X and sheer y force readings to a location of a red circular object in an image, this object then moves about the image depending on current force readings, offsetting the locations of the 16 taxels in the image produces a singel image containg the current state of the sheer force readings. However there are significant issues with this representation. First, the resolution of the image reduces the resolution of the tactile readings. Second the taxel objects cross over, producing an impossible problem for the prediction model to interpret, finally the system presented produced poor test losses. The  model presented, uses a simplified convolutional LSTM chain structure presented in \cite{finn2016unsupervised}.





----------------------------------------------------------------------------------------------------------
@article{johansson2009coding,
  title={Coding and use of tactile signals from the fingertips in object manipulation tasks},
  author={Johansson, Roland S and Flanagan, J Randall},
  journal={Nature Reviews Neuroscience},
  volume={10},
  number={5},
  pages={345--359},
  year={2009},
  publisher={Nature Publishing Group}
}

@inproceedings{tian2019manipulation,
  title={Manipulation by feel: Touch-based control with deep predictive models},
  author={Tian, Stephen and Ebert, Frederik and Jayaraman, Dinesh and Mudigonda, Mayur and Finn, Chelsea and Calandra, Roberto and Levine, Sergey},
  booktitle={2019 International Conference on Robotics and Automation (ICRA)},
  pages={818--824},
  year={2019},
  organization={IEEE}
}

@article{yousef2011tactile,
  title={Tactile sensing for dexterous in-hand manipulation in robotics—A review},
  author={Yousef, Hanna and Boukallel, Mehdi and Althoefer, Kaspar},
  journal={Sensors and Actuators A: physical},
  volume={167},
  number={2},
  pages={171--187},
  year={2011},
  publisher={Elsevier}
}

@article{romeo2020methods,
  title={Methods and Sensors for Slip Detection in Robotics: A Survey},
  author={Romeo, Rocco A and Zollo, Loredana},
  journal={IEEE Access},
  volume={8},
  pages={73027--73050},
  year={2020},
  publisher={IEEE}
}

@article{dahiya2009tactile,
  title={Tactile sensing—from humans to humanoids},
  author={Dahiya, Ravinder S and Metta, Giorgio and Valle, Maurizio and Sandini, Giulio},
  journal={IEEE transactions on robotics},
  volume={26},
  number={1},
  pages={1--20},
  year={2009},
  publisher={IEEE}
}

@article{finn2016unsupervised,
  title={Unsupervised learning for physical interaction through video prediction},
  author={Finn, Chelsea and Goodfellow, Ian and Levine, Sergey},
  journal={arXiv preprint arXiv:1605.07157},
  year={2016}
}

@inproceedings{zhou2020learning,
  title={Learning to Predict Friction and Classify Contact States by Tactile Sensor},
  author={Zhou, Xingru and Zhang, Zheng and Zhu, Xiaojun and Liu, Houde and Liang, Bin},
  booktitle={2020 IEEE 16th International Conference on Automation Science and Engineering (CASE)},
  pages={1243--1248},
  year={2020},
  organization={IEEE}
}










































Prediction withhin the context of tactile sensing produces two questions, what will an object feel like when i grasp it and how will this object feel when I move it? The second is the focus of this paper.  