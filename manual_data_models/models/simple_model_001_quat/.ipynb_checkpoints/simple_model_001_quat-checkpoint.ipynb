{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "mathematical-wagner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import csv\n",
    "import tqdm\n",
    "import copy\n",
    "import click\n",
    "import logging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from string import digits\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "\n",
    "seed = 42\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "learning_rate = 1e-3\n",
    "context_frames = 10\n",
    "sequence_length = 16\n",
    "lookback = sequence_length\n",
    "\n",
    "context_epochs = 20\n",
    "context_batch_size = 1\n",
    "context_learning_rate = 1e-3\n",
    "context_data_length = 20\n",
    "\n",
    "valid_train_split = 0.8  # precentage of train data from total\n",
    "test_train_split = 0.9  # precentage of train data from total\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")#  use gpu if available\n",
    "################################# CHANGE THIS!!!!  #################################\n",
    "model_path = \"/home/user/Robotics/slip_detection_model/slip_detection_model/manual_data_models/models/simple_model_001_quat/\"\n",
    "################################# CHANGE THIS!!!!  #################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "sound-resolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchGenerator:\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        data_map = []\n",
    "        with open(data_dir + 'map.csv', 'r') as f:  # rb\n",
    "            reader = csv.reader(f)\n",
    "            for row in reader:\n",
    "                data_map.append(row)\n",
    "\n",
    "        if len(data_map) <= 1: # empty or only header\n",
    "            print(\"No file map found\")\n",
    "            exit()\n",
    "\n",
    "        self.data_map = data_map\n",
    "\n",
    "    def load_full_data(self):\n",
    "        dataset_train = FullDataSet(self.data_dir, self.data_map, type_=\"train\")\n",
    "        dataset_valid = FullDataSet(self.data_dir, self.data_map, type_=\"valid\")\n",
    "        dataset_test = FullDataSet(self.data_dir, self.data_map, type_=\"test\")\n",
    "        transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "        train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "        valid_loader = torch.utils.data.DataLoader(dataset_valid, batch_size=batch_size, shuffle=True)\n",
    "        test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size, shuffle=False)\n",
    "        return train_loader, valid_loader, test_loader\n",
    "\n",
    "\n",
    "class FullDataSet():\n",
    "    def __init__(self, data_dir, data_map, type_=\"train\"):\n",
    "        if type_ == \"train\":\n",
    "            self.samples = data_map[1:int(len(data_map)*test_train_split)]\n",
    "        elif type_ == \"valid\":\n",
    "            self.samples = data_map[int(len(data_map)*(valid_train_split)):int(len(data_map)*test_train_split)]\n",
    "        elif type_ == \"test\":\n",
    "            self.samples = data_map[int(len(data_map)*test_train_split):-1]\n",
    "        data_map = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        value = self.samples[idx]\n",
    "        robot = np.load(data_dir + value[1])\n",
    "        xela1_data = np.load(data_dir + value[2])\n",
    "        experiment = np.load(data_dir + value[-2])\n",
    "        time_step  = np.load(data_dir + value[-1])     \n",
    "        return([robot.astype(np.float32),\n",
    "                 xela1_data.astype(np.float32),\n",
    "                 experiment,\n",
    "                 time_step])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "reasonable-teens",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FullModel, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(48, 48).to(device)  # tactile\n",
    "        self.lstm2 = nn.LSTM(7, 7).to(device)  # pos_vel\n",
    "#         self.fc11 = nn.Linear(48+7, 48).to(device)  # tactile + pos_vel        \n",
    "        self.lstm3 = nn.LSTM(48+7, 48).to(device)  # pos_vel\n",
    "\n",
    "    def forward(self, tactiles, actions):\n",
    "        state = actions[0]\n",
    "        state.to(device)\n",
    "        batch_size__ = tactiles.shape[1]\n",
    "        outputs = []\n",
    "        hidden1 = (torch.rand(1,batch_size__,48).to(device), torch.rand(1,batch_size__,48).to(device))\n",
    "        hidden2 = (torch.rand(1,batch_size__,7).to(device), torch.rand(1,batch_size__,7).to(device))\n",
    "        hidden3 = (torch.rand(1,batch_size__,48).to(device), torch.rand(1,batch_size__,48).to(device))\n",
    "        for index, (sample_tactile, sample_action) in enumerate(zip(tactiles.squeeze(), actions.squeeze())):\n",
    "            sample_tactile.to(device)\n",
    "            sample_action.to(device)\n",
    "            # 2. Run through lstm:\n",
    "            if index > context_frames-1:\n",
    "                out1, hidden1 = self.lstm1(out4, hidden1)\n",
    "                out2, hidden2 = self.lstm2(sample_action.unsqueeze(0), hidden2)\n",
    "                robot_and_tactile = torch.cat((out2.squeeze(), out1.squeeze()), 1)\n",
    "                out4, hidden3 = self.lstm3(robot_and_tactile.unsqueeze(0), hidden3)\n",
    "                outputs.append(out4.squeeze())\n",
    "            else:\n",
    "                out1, hidden1 = self.lstm1(sample_tactile.unsqueeze(0), hidden1)\n",
    "                out2, hidden2 = self.lstm2(sample_action.unsqueeze(0), hidden2)\n",
    "                robot_and_tactile = torch.cat((out2.squeeze(), out1.squeeze()), 1)\n",
    "                out4, hidden3 = self.lstm3(robot_and_tactile.unsqueeze(0), hidden3)\n",
    "\n",
    "        return torch.stack(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "light-venice",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.train_full_loader, self.valid_full_loader, self.test_full_loader = BG.load_full_data()\n",
    "        self.full_model = FullModel()\n",
    "        self.criterion = nn.L1Loss()\n",
    "        self.optimizer = optim.Adam(self.full_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    def train_full_model(self):\n",
    "        plot_training_loss = []\n",
    "        plot_validation_loss = []\n",
    "        previous_val_mean_loss = 1.0\n",
    "        early_stop_clock = 0\n",
    "        progress_bar = tqdm.tqdm(range(0, epochs), total=(epochs*len(self.train_full_loader)))\n",
    "        mean_test = 0\n",
    "        for epoch in progress_bar:\n",
    "            loss = 0\n",
    "            losses = 0.0\n",
    "            for index, batch_features in enumerate(self.train_full_loader):\n",
    "                action = batch_features[0].permute(1,0,2).to(device)\n",
    "                tactile = batch_features[1].permute(1,0,2).to(device)\n",
    "\n",
    "                tactile_predictions = self.full_model.forward(tactiles=tactile, actions=action) # Step 3. Run our forward pass.\n",
    "                self.optimizer.zero_grad()\n",
    "                loss = self.criterion(tactile_predictions.to(device), tactile[context_frames:])\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                losses += loss.item()\n",
    "                if index:\n",
    "                    mean = losses / index\n",
    "                else:\n",
    "                    mean = 0\n",
    "                progress_bar.set_description(\"epoch: {}, \".format(epoch) + \"loss: {:.4f}, \".format(float(loss.item())) + \"mean loss: {:.4f}, \".format(mean))\n",
    "                progress_bar.update()\n",
    "            plot_training_loss.append(mean)\n",
    "\n",
    "            val_losses = 0.0\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for index__, batch_features in enumerate(self.valid_full_loader):\n",
    "                    action = batch_features[0].permute(1,0,2).to(device)\n",
    "                    tactile = batch_features[1].permute(1,0,2).to(device)\n",
    "\n",
    "                    tactile_predictions = self.full_model.forward(tactiles=tactile, actions=action)  # Step 3. Run our forward pass.\n",
    "                    self.optimizer.zero_grad()\n",
    "                    val_loss = self.criterion(tactile_predictions.to(device), tactile[context_frames:])\n",
    "                    val_losses += val_loss.item()\n",
    "\n",
    "            print(\"Validation mean loss: {:.4f}, \".format(val_losses / index__))\n",
    "            plot_validation_loss.append(val_losses / index__)\n",
    "            if previous_val_mean_loss < val_losses / index__:\n",
    "                early_stop_clock +=1\n",
    "                previous_val_mean_loss = val_losses / index__ \n",
    "                if early_stop_clock == 3:\n",
    "                    print(\"Early stopping\")\n",
    "                    break\n",
    "            else:\n",
    "                self.strongest_model = copy.deepcopy(self.full_model)\n",
    "                early_stop_clock = 0\n",
    "                previous_val_mean_loss = val_losses / index__ \n",
    "        plt.plot(plot_training_loss, c=\"r\", label=\"train loss MAE\")\n",
    "        plt.plot(plot_validation_loss, c='b', label=\"val loss MAE\")\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.show()\n",
    "        np.save(model_path + 'training_loss', np.asarray(plot_training_loss))\n",
    "        np.save(model_path + 'validation_loss', np.asarray(plot_validation_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "facial-voluntary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "data_dir = '/home/user/Robotics/Data_sets/slip_detection/manual_slip_detection/'\n",
    "BG = BatchGenerator(data_dir)\n",
    "MT = ModelTrainer(data_dir)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "contained-breath",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/142100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input must have 3 dimensions, got 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-884e453eded3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mMT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_full_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"finished training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrongest_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"full_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-56-4dc4ceea422d>\u001b[0m in \u001b[0;36mtrain_full_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0mtactile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                 \u001b[0mtactile_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtactiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtactile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Step 3. Run our forward pass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtactile_predictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtactile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcontext_frames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-5a91fd4dcb4d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tactiles, actions)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;31m# 2. Run through lstm:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mcontext_frames\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                 \u001b[0mout1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m                 \u001b[0mout2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_action\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0mrobot_and_tactile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Robotics/slip_detection_model/venv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Robotics/slip_detection_model/venv/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "\u001b[0;32m~/Robotics/slip_detection_model/venv/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m         \u001b[0mexpected_hidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_expected_hidden_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Robotics/slip_detection_model/venv/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_input\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    174\u001b[0m             raise RuntimeError(\n\u001b[1;32m    175\u001b[0m                 'input must have {} dimensions, got {}'.format(\n\u001b[0;32m--> 176\u001b[0;31m                     expected_input_dim, input.dim()))\n\u001b[0m\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             raise RuntimeError(\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input must have 3 dimensions, got 4"
     ]
    }
   ],
   "source": [
    "MT.train_full_model()\n",
    "print(\"finished training\")\n",
    "torch.save(MT.strongest_model, model_path + \"full_model\")\n",
    "model = torch.load(model_path)\n",
    "model.eval()\n",
    "print(\"saved the model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesbian-delay",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model on the full test sample:\n",
    "# model = MT.strongest_model\n",
    "data_dir = MT.data_dir\n",
    "\n",
    "criterion1 = nn.L1Loss()\n",
    "criterion2 = nn.MSELoss()\n",
    "\n",
    "test_lossesMAE_x = 0.0\n",
    "test_lossesMSE_x = 0.0\n",
    "test_lossesMAE_y = 0.0\n",
    "test_lossesMSE_y = 0.0\n",
    "test_lossesMAE_z = 0.0\n",
    "test_lossesMSE_z = 0.0\n",
    "\n",
    "test_lossesMAE_t1 = 0.0\n",
    "test_lossesMSE_t1 = 0.0\n",
    "test_lossesMAE_t5 = 0.0\n",
    "test_lossesMSE_t5 = 0.0\n",
    "test_lossesMAE_t10 = 0.0\n",
    "test_lossesMSE_t10 = 0.0\n",
    "\n",
    "test_lossesMAE_x_ts1 = 0.0\n",
    "test_lossesMSE_x_ts1 = 0.0\n",
    "test_lossesMAE_y_ts1 = 0.0\n",
    "test_lossesMSE_y_ts1 = 0.0\n",
    "test_lossesMAE_z_ts1 = 0.0\n",
    "test_lossesMSE_z_ts1 = 0.0\n",
    "\n",
    "test_lossesMAE_x_ts5 = 0.0\n",
    "test_lossesMSE_x_ts5 = 0.0\n",
    "test_lossesMAE_y_ts5 = 0.0\n",
    "test_lossesMSE_y_ts5 = 0.0\n",
    "test_lossesMAE_z_ts5 = 0.0\n",
    "test_lossesMSE_z_ts5 = 0.0\n",
    "\n",
    "test_lossesMAE_x_ts10 = 0.0\n",
    "test_lossesMSE_x_ts10 = 0.0\n",
    "test_lossesMAE_y_ts10 = 0.0\n",
    "test_lossesMSE_y_ts10 = 0.0\n",
    "test_lossesMAE_z_ts10 = 0.0\n",
    "test_lossesMSE_z_ts10 = 0.0\n",
    "\n",
    "tactile_predictions = []\n",
    "tactile_groundtruth = []\n",
    "experiment_time_steps = []\n",
    "test_lossesMAE = 0.0\n",
    "test_lossesMSE = 0.0\n",
    "with torch.no_grad():\n",
    "    for index__, batch_features in enumerate(MT.test_full_loader):\n",
    "        # 2. Reshape data and send to device:\n",
    "        action = batch_features[0].permute(1,0,2).to(device)\n",
    "        tactile = batch_features[1].permute(1,0,2).to(device)\n",
    "\n",
    "        tp = model.forward(tactiles=tactile, actions=action)\n",
    "        experiment_time_steps.append([batch_features[3], batch_features[4]])\n",
    "        tactile_predictions.append(tp)  # Step 3. Run our forward pass.\n",
    "        tactile_groundtruth.append(tactile[context_frames:])\n",
    "        # calculate losses for specific timesteps\n",
    "        test_lossMAE_t1 = criterion1(tp[0,:,:].to(device), tactile[context_frames:][0,:,:])\n",
    "        test_lossesMAE_t1 += test_lossMAE_t1.item() \n",
    "        test_lossMSE_t1 = criterion2(tp[0,:,:].to(device), tactile[context_frames:][0,:,:])\n",
    "        test_lossesMSE_t1 += test_lossMSE_t1.item() \n",
    "        test_lossMAE_t5 = criterion1(tp[4,:,:].to(device), tactile[context_frames:][4,:,:])\n",
    "        test_lossesMAE_t5 += test_lossMAE_t5.item() \n",
    "        test_lossMSE_t5 = criterion2(tp[4,:,:].to(device), tactile[context_frames:][4,:,:])\n",
    "        test_lossesMSE_t5 += test_lossMSE_t5.item() \n",
    "        test_lossMAE_t10 = criterion1(tp[9,:,:].to(device), tactile[context_frames:][9,:,:])\n",
    "        test_lossesMAE_t10 += test_lossMAE_t10.item() \n",
    "        test_lossMSE_t10 = criterion2(tp[9,:,:].to(device), tactile[context_frames:][9,:,:])\n",
    "        test_lossesMSE_t10 += test_lossMSE_t10.item() \n",
    "        \n",
    "        # calculate losses for specific forces\n",
    "        test_lossMAE_x = criterion1(tp[:,:,:16].to(device), tactile[context_frames:][:,:,:16])\n",
    "        test_lossesMAE_x += test_lossMAE_x.item() \n",
    "        test_lossMSE_x = criterion2(tp[:,:,:16].to(device), tactile[context_frames:][:,:,:16])\n",
    "        test_lossesMSE_x += test_lossMSE_x.item() \n",
    "        test_lossMAE_y = criterion1(tp[:,:,17:32].to(device), tactile[context_frames:][:,:,17:32])\n",
    "        test_lossesMAE_y += test_lossMAE_y.item() \n",
    "        test_lossMSE_y = criterion2(tp[:,:,17:32].to(device), tactile[context_frames:][:,:,17:32])\n",
    "        test_lossesMSE_y += test_lossMSE_y.item() \n",
    "        test_lossMAE_z = criterion1(tp[:,:,33:48].to(device), tactile[context_frames:][:,:,33:48])\n",
    "        test_lossesMAE_z += test_lossMAE_z.item() \n",
    "        test_lossMSE_z = criterion2(tp[:,:,33:48].to(device), tactile[context_frames:][:,:,33:48])\n",
    "        test_lossesMSE_z += test_lossMSE_z.item() \n",
    "\n",
    "        # calculate losses for specific timesteps and forces \n",
    "        test_lossMAE_x_ts1 = criterion1(tp[0,:,:16].to(device), tactile[context_frames:][0,:,:16])\n",
    "        test_lossesMAE_x_ts1 += test_lossMAE_x_ts1.item() \n",
    "        test_lossMSE_x_ts1 = criterion2(tp[0,:,:16].to(device), tactile[context_frames:][0,:,:16])\n",
    "        test_lossesMSE_x_ts1 += test_lossMSE_x_ts1.item() \n",
    "        test_lossMAE_y_ts1 = criterion1(tp[0,:,17:32].to(device), tactile[context_frames:][0,:,17:32])\n",
    "        test_lossesMAE_y_ts1 += test_lossMAE_y_ts1.item() \n",
    "        test_lossMSE_y_ts1 = criterion2(tp[0,:,17:32].to(device), tactile[context_frames:][0,:,17:32])\n",
    "        test_lossesMSE_y_ts1 += test_lossMSE_y_ts1.item() \n",
    "        test_lossMAE_z_ts1 = criterion1(tp[0,:,33:48].to(device), tactile[context_frames:][0,:,33:48])\n",
    "        test_lossesMAE_z_ts1 += test_lossMAE_z_ts1.item() \n",
    "        test_lossMSE_z_ts1 = criterion2(tp[0,:,33:48].to(device), tactile[context_frames:][0,:,33:48])\n",
    "        test_lossesMSE_z_ts1 += test_lossMSE_z_ts1.item() \n",
    " \n",
    "        test_lossMAE_x_ts5 = criterion1(tp[4,:,:16].to(device), tactile[context_frames:][4,:,:16])\n",
    "        test_lossesMAE_x_ts5 += test_lossMAE_x_ts5.item() \n",
    "        test_lossMSE_x_ts5 = criterion2(tp[4,:,:16].to(device), tactile[context_frames:][4,:,:16])\n",
    "        test_lossesMSE_x_ts5 += test_lossMSE_x_ts5.item() \n",
    "        test_lossMAE_y_ts5 = criterion1(tp[4,:,17:32].to(device), tactile[context_frames:][4,:,17:32])\n",
    "        test_lossesMAE_y_ts5 += test_lossMAE_y_ts5.item() \n",
    "        test_lossMSE_y_ts5 = criterion2(tp[4,:,17:32].to(device), tactile[context_frames:][4,:,17:32])\n",
    "        test_lossesMSE_y_ts5 += test_lossMSE_y_ts5.item() \n",
    "        test_lossMAE_z_ts5 = criterion1(tp[4,:,33:48].to(device), tactile[context_frames:][4,:,33:48])\n",
    "        test_lossesMAE_z_ts5 += test_lossMAE_z_ts5.item() \n",
    "        test_lossMSE_z_ts5 = criterion2(tp[4,:,33:48].to(device), tactile[context_frames:][4,:,33:48])\n",
    "        test_lossesMSE_z_ts5 += test_lossMSE_z_ts5.item() \n",
    "\n",
    "        test_lossMAE_x_ts10 = criterion1(tp[9,:,:16].to(device), tactile[context_frames:][9,:,:16])\n",
    "        test_lossesMAE_x_ts10 += test_lossMAE_x_ts10.item() \n",
    "        test_lossMSE_x_ts10 = criterion2(tp[9,:,:16].to(device), tactile[context_frames:][9,:,:16])\n",
    "        test_lossesMSE_x_ts10 += test_lossMSE_x_ts10.item() \n",
    "        test_lossMAE_y_ts10 = criterion1(tp[9,:,17:32].to(device), tactile[context_frames:][9,:,17:32])\n",
    "        test_lossesMAE_y_ts10 += test_lossMAE_y_ts10.item() \n",
    "        test_lossMSE_y_ts10 = criterion2(tp[9,:,17:32].to(device), tactile[context_frames:][9,:,17:32])\n",
    "        test_lossesMSE_y_ts10 += test_lossMSE_y_ts10.item() \n",
    "        test_lossMAE_z_ts10 = criterion1(tp[9,:,33:48].to(device), tactile[context_frames:][9,:,33:48])\n",
    "        test_lossesMAE_z_ts10 += test_lossMAE_z_ts10.item() \n",
    "        test_lossMSE_z_ts10 = criterion2(tp[9,:,33:48].to(device), tactile[context_frames:][9,:,33:48])\n",
    "        test_lossesMSE_z_ts10 += test_lossMSE_z_ts10.item()\n",
    "\n",
    "performance_data = []\n",
    "performance_data.append([\"test loss MAE(L1): \", (test_lossesMAE / index__)])\n",
    "performance_data.append([\"test loss MSE: \", (test_lossesMSE / index__)])\n",
    "performance_data.append([\"test loss MAE(L1) timestep 1: \", (test_lossesMAE_x / index__)])\n",
    "performance_data.append([\"test loss MSE timestep 1: \", (test_lossesMSE_x / index__)])\n",
    "performance_data.append([\"test loss MAE(L1) timestep 5: \", (test_lossesMAE_y / index__)])\n",
    "performance_data.append([\"test loss MSE timestep 5: \", (test_lossesMSE_y / index__)])\n",
    "performance_data.append([\"test loss MAE(L1) timestep 10: \", (test_lossesMAE_z / index__)])\n",
    "performance_data.append([\"test loss MSE timestep 10: \", (test_lossesMSE_z / index__)])\n",
    "performance_data.append([\"sheer x test loss MAE(L1): \", (test_lossesMAE_x / index__)])\n",
    "performance_data.append([\"sheer x test loss MSE: \", (test_lossesMSE_x / index__)])\n",
    "performance_data.append([\"sheer y test loss MAE(L1): \", (test_lossesMAE_y / index__)])\n",
    "performance_data.append([\"sheer y test loss MSE: \", (test_lossesMSE_y / index__)])\n",
    "performance_data.append([\"z test loss MAE(L1): \", (test_lossesMAE_z / index__)])\n",
    "performance_data.append([\"z test loss MSE: \", (test_lossesMSE_z / index__)])\n",
    "performance_data.append([\"sheer x test loss MAE(L1) timestep 1: \", (test_lossesMAE_x_ts1 / index__)])\n",
    "performance_data.append([\"sheer x test loss MSE timestep 1: \", (test_lossesMSE_x_ts1 / index__)])\n",
    "performance_data.append([\"sheer y test loss MAE(L1) timestep 1: \", (test_lossesMAE_y_ts1 / index__)])\n",
    "performance_data.append([\"sheer y test loss MSE timestep 1: \", (test_lossesMSE_y_ts1 / index__)])\n",
    "performance_data.append([\"z test loss MAE(L1) timestep 1: \", (test_lossesMAE_z_ts1 / index__)])\n",
    "performance_data.append([\"z test loss MSE timestep 1: \", (test_lossesMSE_z_ts1 / index__)])\n",
    "performance_data.append([\"sheer x test loss MAE(L1) timestep 5: \", (test_lossesMAE_x_ts5 / index__)])\n",
    "performance_data.append([\"sheer x test loss MSE timestep 5: \", (test_lossesMSE_x_ts5 / index__)])\n",
    "performance_data.append([\"sheer y test loss MAE(L1) timestep 5: \", (test_lossesMAE_y_ts5 / index__)])\n",
    "performance_data.append([\"sheer y test loss MSE timestep 5: \", (test_lossesMSE_y_ts5 / index__)])\n",
    "performance_data.append([\"z test loss MAE(L1) timestep 5: \", (test_lossesMAE_z_ts5 / index__)])\n",
    "performance_data.append([\"z test loss MSE timestep 5: \", (test_lossesMSE_z_ts5 / index__)])\n",
    "performance_data.append([\"sheer x test loss MAE(L1) timestep 10: \", (test_lossesMAE_x_ts9 / index__)])\n",
    "performance_data.append([\"sheer x test loss MSE timestep 10: \", (test_lossesMSE_x_ts9 / index__)])\n",
    "performance_data.append([\"sheer y test loss MAE(L1) timestep 10: \", (test_lossesMAE_y_ts9 / index__)])\n",
    "performance_data.append([\"sheer y test loss MSE timestep 10: \", (test_lossesMSE_y_ts9 / index__)])\n",
    "performance_data.append([\"z test loss MAE(L1) timestep 10: \", (test_lossesMAE_z_ts9 / index__)])\n",
    "performance_data.append([\"z test loss MSE timestep 10: \", (test_lossesMSE_z_ts9 / index__)])\n",
    "[print(i) for i in performance_data]\n",
    "\n",
    "np.save(model_path + 'performance_data', np.asarray(performance_data))\n",
    "\n",
    "# calculate tactile values for full sample:\n",
    "time_step_to_test_t1 = 0    # [batch_set, prediction frames(t1->tx)(6), batch_size, features(48)]\n",
    "time_step_to_test_t9 = 5\n",
    "predicted_data_t1 = []\n",
    "predicted_data_t9 = []\n",
    "groundtruth_data = []\n",
    "for index, batch_set in enumerate(tactile_predictions):\n",
    "    for batch in range(0, len(batch_set[0])):\n",
    "        prediction_values = batch_set[time_step_to_test_t1][batch]\n",
    "        predicted_data_t1.append(prediction_values)\n",
    "        prediction_values = batch_set[time_step_to_test_t9][batch]\n",
    "        predicted_data_t9.append(prediction_values)\n",
    "        gt_values = tactile_groundtruth[index][time_step_to_test_t1][batch]\n",
    "        groundtruth_data.append(gt_values)  \n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "listed-allah",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate tactile values for full sample:\n",
    "time_step_to_test_t1 = 0    # [batch_set, prediction frames(t1->tx)(6), batch_size, features(48)]\n",
    "time_step_to_test_t5 = 4\n",
    "time_step_to_test_t9 = 9\n",
    "predicted_data_t1 = []\n",
    "predicted_data_t5 = []\n",
    "predicted_data_t9 = []\n",
    "groundtruth_data = []\n",
    "experiment_to_test = 106\n",
    "for index, batch_set in enumerate(tactile_predictions):\n",
    "    for batch in range(0, len(batch_set[0])):\n",
    "        experiment = experiment_time_steps[index][0][batch]\n",
    "        if experiment == experiment_to_test:\n",
    "            prediction_values = batch_set[time_step_to_test_t1][batch]\n",
    "            predicted_data_t1.append(prediction_values)\n",
    "            prediction_values = batch_set[time_step_to_test_t5][batch]\n",
    "            predicted_data_t5.append(prediction_values)\n",
    "            prediction_values = batch_set[time_step_to_test_t9][batch]\n",
    "            predicted_data_t9.append(prediction_values)\n",
    "            gt_values = tactile_groundtruth[index][time_step_to_test_t1][batch]\n",
    "            groundtruth_data.append(gt_values)\n",
    "\n",
    "# print(tactile_predictions[0])\n",
    "# plt.plot([i for i in range(len(tactile_predictions[0]))], [i for i in range(len(tactile_predictions[0]))])\n",
    "plt.show()        \n",
    "mse_loss = torch.nn.MSELoss()\n",
    "# print(\"MAE timestep + 1: \", np.mean(np.asarray([mse_loss(np.asarray(pred.cpu().detach()), np.asarray(gt.cpu().detach()))  for pred, gt in zip(predicted_data_t1, groundtruth_data)])))\n",
    "# print(\"MAE timestep + 5: \", mse_loss(torch.tensor(predicted_data_t5), torch.tensor(groundtruth_data)))\n",
    "# print(\"MAE timestep + 9: \", mse_loss(torch.tensor(predicted_data_t9), torch.tensor(groundtruth_data)))\n",
    "\n",
    "# test data\n",
    "index = 0\n",
    "titles = [\"sheerx\", \"sheery\", \"normal\"]\n",
    "for j in range(3):\n",
    "    for i in range(16):\n",
    "        groundtruth_taxle = []\n",
    "        predicted_taxel = []\n",
    "        predicted_taxel_t1 = []\n",
    "        predicted_taxel_t5 = []\n",
    "        predicted_taxel_t9 = []\n",
    "        # good = 140, 145 (lifting up the )\n",
    "        for k in range(len(predicted_data_t1)):#310, 325):#len(predicted_data_t1)):  # add in length of context data\n",
    "            predicted_taxel_t1.append(predicted_data_t1[k][j+i].cpu().detach().numpy())\n",
    "            predicted_taxel_t5.append(predicted_data_t5[k][j+i].cpu().detach().numpy())\n",
    "            predicted_taxel_t9.append(predicted_data_t9[k][j+i].cpu().detach().numpy())\n",
    "            groundtruth_taxle.append(groundtruth_data[k][j+i].cpu().detach().numpy())\n",
    "\n",
    "        index += 1\n",
    "        fig, ax1 = plt.subplots()\n",
    "        ax1.set_xlabel('time step')\n",
    "        ax1.set_ylabel('tactile reading')\n",
    "        ax1.plot(predicted_taxel_t1, alpha=0.5, c=\"b\", label=\"t1\")\n",
    "        ax1.plot(predicted_taxel_t5, alpha=0.5, c=\"k\", label=\"t5\")\n",
    "        ax1.plot(predicted_taxel_t9, alpha=0.5, c=\"g\", label=\"t10\")\n",
    "        ax1.plot(groundtruth_taxle, alpha=0.5, c=\"r\", label=\"gt\")\n",
    "        ax1.tick_params(axis='y')\n",
    "        ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "        ax2.set_ylabel('loss')  # we already handled the x-label with ax1\n",
    "        ax2.plot([i for i in range(len(groundtruth_data))], [mse_loss(predicted_data_t9[i], groundtruth_data[i]) for i in range(len(groundtruth_data))], alpha=0.5)\n",
    "        fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "        fig.subplots_adjust(top=0.90)\n",
    "        ax1.legend(loc=\"upper right\")\n",
    "        plt.title(\"Simple_LSTM tactile \" + str(index))\n",
    "        plt.savefig(model_path + '/' + str(experiment_to_test) + '/sample_test_with_loss_' + str(index) + '.png', dpi=300)\n",
    "        plt.show()\n",
    "\n",
    "        fig, ax1 = plt.subplots()\n",
    "        ax1.set_xlabel('time step')\n",
    "        ax1.set_ylabel('tactile reading')\n",
    "        ax1.plot(predicted_taxel_t1, alpha=0.5, c=\"b\", label=\"t1\")\n",
    "        ax1.plot(groundtruth_taxle, alpha=0.5, c=\"r\", label=\"gt\")\n",
    "        ax1.tick_params(axis='y')\n",
    "        ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "        ax2.set_ylabel('loss')  # we already handled the x-label with ax1\n",
    "        ax2.plot([i for i in range(len(groundtruth_data))], [mse_loss(predicted_data_t9[i], groundtruth_data[i]) for i in range(len(groundtruth_data))], alpha=0.5)\n",
    "        fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "        fig.subplots_adjust(top=0.90)\n",
    "        ax1.legend(loc=\"upper right\")\n",
    "        plt.title(\"Simple_LSTM tactile \" + str(index))\n",
    "        plt.savefig(model_path + '/' + str(experiment_to_test) + '/T0sample_test_with_loss_' + str(index) + '.png', dpi=300)\n",
    "        plt.show()\n",
    "        \n",
    "        fig, ax1 = plt.subplots()\n",
    "        ax1.set_xlabel('time step')\n",
    "        ax1.set_ylabel('tactile reading')\n",
    "        ax1.plot(predicted_taxel_t5, alpha=0.5, c=\"b\", label=\"t5\")\n",
    "        ax1.plot(groundtruth_taxle, alpha=0.5, c=\"r\", label=\"gt\")\n",
    "        ax1.tick_params(axis='y')\n",
    "        ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "        ax2.set_ylabel('loss')  # we already handled the x-label with ax1\n",
    "        ax2.plot([i for i in range(len(groundtruth_data))], [mse_loss(predicted_data_t9[i], groundtruth_data[i]) for i in range(len(groundtruth_data))], alpha=0.5)\n",
    "        fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "        fig.subplots_adjust(top=0.90)\n",
    "        ax1.legend(loc=\"upper right\")\n",
    "        plt.title(\"Simple_LSTM tactile \" + str(index))\n",
    "        plt.savefig(model_path + '/' + str(experiment_to_test) + '/T5sample_test_with_loss_' + str(index) + '.png', dpi=300)\n",
    "        plt.show()\n",
    "            \n",
    "        fig, ax1 = plt.subplots()\n",
    "        ax1.set_xlabel('time step')\n",
    "        ax1.set_ylabel('tactile reading')\n",
    "        ax1.plot(predicted_taxel_t9, alpha=0.5, c=\"b\", label=\"t10\")\n",
    "        ax1.plot(groundtruth_taxle, alpha=0.5, c=\"r\", label=\"gt\")\n",
    "        ax1.tick_params(axis='y')\n",
    "        ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "        ax2.set_ylabel('loss')  # we already handled the x-label with ax1\n",
    "        ax2.plot([i for i in range(len(groundtruth_data))], [mse_loss(predicted_data_t9[i], groundtruth_data[i]) for i in range(len(groundtruth_data))], alpha=0.5)\n",
    "        fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "        fig.subplots_adjust(top=0.90)\n",
    "        ax1.legend(loc=\"upper right\")\n",
    "        plt.title(\"Simple_LSTM tactile \" + str(index))\n",
    "        plt.savefig(model_path + '/' + str(experiment_to_test) + '/T10sample_test_with_loss_' + str(index) + '.png', dpi=300)\n",
    "        plt.show()\n",
    "    \n",
    "#         plt.plot(predicted_taxel_t1, alpha=0.5, c=\"b\", label=\"t0\")\n",
    "#         plt.plot(predicted_taxel_t5, alpha=0.5, c=\"k\", label=\"t5\")\n",
    "#         plt.plot(predicted_taxel_t9, alpha=0.5, c=\"g\", label=\"t9\")\n",
    "#         plt.plot(groundtruth_taxle, alpha=0.5, c=\"r\", label=\"gt\")\n",
    "#         plt.ylim([0, 1])\n",
    "#         plt.grid()\n",
    "#         plt.legend(loc=\"upper right\")\n",
    "#         plt.savefig('/home/user/Pictures/simple_model_001/simple_model_test_sample_' + str(index) + '.png')\n",
    "#         plt.show()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
