{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "utility-spanish",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import csv\n",
    "import tqdm\n",
    "import copy\n",
    "import click\n",
    "import logging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from string import digits\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "\n",
    "seed = 42\n",
    "epochs = 100\n",
    "batch_size = 80\n",
    "learning_rate = 1e-3\n",
    "context_frames = 10\n",
    "sequence_length = 20\n",
    "lookback = sequence_length\n",
    "\n",
    "valid_train_split = 0.85  # precentage of train data from total\n",
    "test_train_split = 0.92  # precentage of train data from total\n",
    "\n",
    "image_height = 32\n",
    "image_width  = 32\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")#  use gpu if available\n",
    "################################# CHANGE THIS!!!!  #################################\n",
    "model_path = \"/home/user/Robotics/slip_detection_model/slip_detection_model/manual_data_models/models/CorlPaperFinal/conv_model/\"\n",
    "data_dir = \"/home/user/Robotics/Data_sets/slip_detection/will_dataset/will_data_collection/formated_image_data/\"\n",
    "################################# CHANGE THIS!!!!  #################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "forbidden-protocol",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchGenerator:\n",
    "    def __init__(self):\n",
    "        self.data_dir = data_dir\n",
    "        data_map = []\n",
    "        with open(data_dir + 'map.csv', 'r') as f:  # rb\n",
    "            reader = csv.reader(f)\n",
    "            for row in reader:\n",
    "                data_map.append(row)\n",
    "\n",
    "        if len(data_map) <= 1: # empty or only header\n",
    "            print(\"No file map found\")\n",
    "            exit()\n",
    "\n",
    "        self.data_map = data_map\n",
    "\n",
    "    def load_full_data(self):\n",
    "        dataset_train = FullDataSet(self.data_dir, self.data_map, type_=\"train\")\n",
    "        dataset_valid = FullDataSet(self.data_dir, self.data_map, type_=\"valid\")\n",
    "        dataset_test = FullDataSet(self.data_dir, self.data_map, type_=\"test\")\n",
    "        transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "        train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "        valid_loader = torch.utils.data.DataLoader(dataset_valid, batch_size=batch_size, shuffle=True)\n",
    "        test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size, shuffle=False)\n",
    "        return train_loader, valid_loader, test_loader\n",
    "\n",
    "class FullDataSet():\n",
    "    def __init__(self, data_dir, data_map, type_=\"train\"):\n",
    "        if type_ == \"train\":\n",
    "            self.samples = data_map[1:int(len(data_map)*test_train_split)]\n",
    "        elif type_ == \"valid\":\n",
    "            self.samples = data_map[int(len(data_map)*(valid_train_split)):int(len(data_map)*test_train_split)]\n",
    "        elif type_ == \"test\":\n",
    "            self.samples = data_map[int(len(data_map)*test_train_split):-1]\n",
    "        data_map = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        # neet to scale between 0 and 1:\n",
    "        value = self.samples[idx]\n",
    "        robot = np.load(data_dir + value[0])\n",
    "        xela1image = [np.load(data_dir + value[i]).squeeze() for i in range(1, sequence_length+1)]\n",
    "        experiment = np.load(data_dir + value[-2])\n",
    "        time_step  = np.load(data_dir + value[-1])\n",
    "        return([robot.astype(np.float32),\n",
    "                np.array(xela1image).astype(np.float32),\n",
    "                experiment,\n",
    "                time_step])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "photographic-financing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as f\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n",
    "        self.bias = bias\n",
    "        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim, out_channels=4 * self.hidden_dim,\n",
    "                              kernel_size=self.kernel_size, padding=self.padding, bias=self.bias)\n",
    "\n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        h_cur, c_cur = cur_state\n",
    "        combined = torch.cat([input_tensor, h_cur], dim=1)  # concatenate along channel axis\n",
    "        combined_conv = self.conv(combined)\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "        c_next = f * c_cur + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "        return h_next, c_next\n",
    "\n",
    "    def init_hidden(self, batch_size, image_size):\n",
    "        height, width = image_size\n",
    "        return (torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device).to(device),\n",
    "                torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device).to(device))\n",
    "\n",
    "# initial test:\n",
    "class FullModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FullModel, self).__init__()\n",
    "        self.convlstm1 = ConvLSTMCell(input_dim=3, hidden_dim=12, kernel_size=(3, 3), bias=True).cuda()\n",
    "        self.convlstm2 = ConvLSTMCell(input_dim=24, hidden_dim=24, kernel_size=(3, 3), bias=True).cuda()\n",
    "        self.conv1 = nn.Conv2d(in_channels=24, out_channels=3, kernel_size=5, stride=1, padding=2).cuda()\n",
    "\n",
    "    def forward(self, tactiles, actions):\n",
    "        self.batch_size = actions.shape[1]\n",
    "        state = actions[0]\n",
    "        state.to(device)\n",
    "        batch_size__ = tactiles.shape[1]\n",
    "        hidden_1, cell_1 = self.convlstm1.init_hidden(batch_size=self.batch_size, image_size=(32, 32))\n",
    "        hidden_2, cell_2 = self.convlstm2.init_hidden(batch_size=self.batch_size, image_size=(32, 32))\n",
    "        outputs = []\n",
    "        for index, (sample_tactile, sample_action) in enumerate(zip(tactiles.squeeze(), actions.squeeze())):\n",
    "            sample_tactile.to(device)\n",
    "            sample_action.to(device)\n",
    "            # 2. Run through lstm:\n",
    "            if index > context_frames-1:\n",
    "                hidden_1, cell_1 = self.convlstm1(input_tensor=output, cur_state=[hidden_1, cell_1])\n",
    "                state_action = torch.cat((state, sample_action), 1)\n",
    "                robot_and_tactile = torch.cat((torch.cat(32*[torch.cat(32*[state_action.unsqueeze(2)], axis = 2).unsqueeze(3)], axis = 3), hidden_1.squeeze()), 1)\n",
    "                hidden_2, cell_2 = self.convlstm2(input_tensor=robot_and_tactile, cur_state=[hidden_2, cell_2])\n",
    "                output = self.conv1(hidden_2)\n",
    "                outputs.append(output)\n",
    "            else:\n",
    "                hidden_1, cell_1 = self.convlstm1(input_tensor=sample_tactile, cur_state=[hidden_1, cell_1])\n",
    "                state_action = torch.cat((state, sample_action), 1)\n",
    "                robot_and_tactile = torch.cat((torch.cat(32*[torch.cat(32*[state_action.unsqueeze(2)], axis = 2).unsqueeze(3)], axis = 3), hidden_1.squeeze()), 1)\n",
    "                hidden_2, cell_2 = self.convlstm2(input_tensor=robot_and_tactile, cur_state=[hidden_2, cell_2])\n",
    "                output = self.conv1(hidden_2)\n",
    "        return torch.stack(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "forced-toddler",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self):\n",
    "        self.train_full_loader, self.valid_full_loader, self.test_full_loader = BG.load_full_data()\n",
    "        self.full_model = FullModel()\n",
    "        self.criterion = nn.L1Loss()\n",
    "        self.criterion1 = nn.L1Loss()\n",
    "        self.optimizer = optim.Adam(self.full_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    def train_full_model(self):\n",
    "        plot_training_loss = []\n",
    "        plot_validation_loss = []\n",
    "        previous_val_mean_loss = 100.0\n",
    "        best_val_loss = 100.0\n",
    "        early_stop_clock = 0\n",
    "        progress_bar = tqdm.tqdm(range(0, epochs), total=(epochs*len(self.train_full_loader)))\n",
    "        mean_test = 0\n",
    "        for epoch in progress_bar:\n",
    "            loss = 0.0\n",
    "            losses = 0.0\n",
    "            for index, batch_features in enumerate(self.train_full_loader):\n",
    "                action = batch_features[0].squeeze().permute(1,0,2).to(device)\n",
    "                tactile = batch_features[1].permute(1,0,4,3,2).to(device)\n",
    "                tactile_predictions = self.full_model.forward(tactiles=tactile, actions=action) # Step 3. Run our forward pass.\n",
    "                self.optimizer.zero_grad()\n",
    "                loss = self.criterion(tactile_predictions, tactile[context_frames:])\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                losses += loss.item()\n",
    "                if index:\n",
    "                    mean = losses / index\n",
    "                else:\n",
    "                    mean = 0\n",
    "                progress_bar.set_description(\"epoch: {}, \".format(epoch) + \"loss: {:.4f}, \".format(float(loss.item())) + \"mean loss: {:.4f}, \".format(mean))\n",
    "                progress_bar.update()\n",
    "            plot_training_loss.append(mean)\n",
    "\n",
    "            val_losses = 0.0\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for index__, batch_features in enumerate(self.valid_full_loader):\n",
    "                    action = batch_features[0].squeeze().permute(1,0,2).to(device)\n",
    "                    tactile = batch_features[1].permute(1,0,4,3,2).to(device)\n",
    "                    tactile_predictions = self.full_model.forward(tactiles=tactile, actions=action) # Step 3. Run our forward pass.                    \n",
    "                    self.optimizer.zero_grad()\n",
    "                    val_loss = self.criterion(tactile_predictions.to(device), tactile[context_frames:])\n",
    "                    val_losses += val_loss.item()\n",
    "\n",
    "            print(\"Validation mean loss: {:.4f}, \".format(val_losses / index__))\n",
    "            plot_validation_loss.append(val_losses / index__)\n",
    "            if previous_val_mean_loss < val_losses / index__:\n",
    "                early_stop_clock +=1\n",
    "                previous_val_mean_loss = val_losses / index__ \n",
    "                if early_stop_clock == 3:\n",
    "                    print(\"Early stopping\")\n",
    "                    break\n",
    "            else:\n",
    "                if best_val_loss > val_losses / index__:\n",
    "                    print(\"saving model\")\n",
    "                    torch.save(self.full_model, model_path + \"conv_model\")\n",
    "                    self.strongest_model = copy.deepcopy(self.full_model)\n",
    "                    best_val_loss = val_losses / index__\n",
    "                early_stop_clock = 0\n",
    "                previous_val_mean_loss = val_losses / index__ \n",
    "            plt.plot(plot_training_loss, c=\"r\", label=\"train loss MAE\")\n",
    "            plt.plot(plot_validation_loss, c='b', label=\"val loss MAE\")\n",
    "            plt.legend(loc=\"upper right\")\n",
    "            plt.savefig(model_path + '/trining_plot_10steppred_new_data.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-problem",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.0191, mean loss: 0.0277, :   1%|          | 2249/224800 [27:43<1500:54:39, 24.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mean loss: 0.0194, \n",
      "saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 0.0125, mean loss: 0.0157, :   2%|▏         | 4497/224800 [53:42<40:01:31,  1.53it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mean loss: 0.0142, \n",
      "saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 2, loss: 0.0135, mean loss: 0.0139, :   3%|▎         | 6745/224800 [1:21:01<39:02:20,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mean loss: 0.0144, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 3, loss: 0.0120, mean loss: 0.0131, :   4%|▍         | 8993/224800 [1:48:25<40:24:09,  1.48it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mean loss: 0.0127, \n",
      "saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 4, loss: 0.0134, mean loss: 0.0124, :   5%|▌         | 11241/224800 [2:15:58<39:19:49,  1.51it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mean loss: 0.0113, \n",
      "saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 5, loss: 0.0109, mean loss: 0.0120, :   6%|▌         | 13489/224800 [2:43:45<39:39:43,  1.48it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mean loss: 0.0109, \n",
      "saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 6, loss: 0.0109, mean loss: 0.0116, :   7%|▋         | 15737/224800 [3:11:07<37:50:58,  1.53it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mean loss: 0.0112, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 7, loss: 0.0104, mean loss: 0.0113, :   8%|▊         | 17985/224800 [3:38:20<37:46:28,  1.52it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mean loss: 0.0107, \n",
      "saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 8, loss: 0.0098, mean loss: 0.0110, :   9%|▉         | 20233/224800 [4:05:29<37:02:50,  1.53it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mean loss: 0.0104, \n",
      "saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 9, loss: 0.0116, mean loss: 0.0107, :  10%|█         | 22481/224800 [4:32:32<36:33:54,  1.54it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mean loss: 0.0121, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 10, loss: 0.0101, mean loss: 0.0105, :  11%|█         | 24729/224800 [4:59:41<36:30:44,  1.52it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mean loss: 0.0114, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 11, loss: 0.0092, mean loss: 0.0103, :  12%|█▏        | 26977/224800 [5:26:47<35:50:07,  1.53it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mean loss: 0.0100, \n",
      "saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 12, loss: 0.0110, mean loss: 0.0101, :  13%|█▎        | 29225/224800 [5:53:54<35:27:33,  1.53it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mean loss: 0.0102, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 13, loss: 0.0101, mean loss: 0.0099, :  14%|█▍        | 31473/224800 [6:21:00<34:55:15,  1.54it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mean loss: 0.0095, \n",
      "saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 14, loss: 0.0106, mean loss: 0.0098, :  15%|█▌        | 33721/224800 [6:48:07<34:31:37,  1.54it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mean loss: 0.0098, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 15, loss: 0.0096, mean loss: 0.0096, :  16%|█▌        | 35969/224800 [7:15:18<34:08:12,  1.54it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mean loss: 0.0094, \n",
      "saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 16, loss: 0.0092, mean loss: 0.0095, :  17%|█▋        | 38217/224800 [7:42:27<33:50:45,  1.53it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mean loss: 0.0094, \n",
      "saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 17, loss: 0.0109, mean loss: 0.0094, :  18%|█▊        | 40465/224800 [8:09:34<33:26:37,  1.53it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mean loss: 0.0093, \n",
      "saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 18, loss: 0.0082, mean loss: 0.0093, :  19%|█▉        | 42713/224800 [8:36:46<33:03:25,  1.53it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mean loss: 0.0092, \n",
      "saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 19, loss: 0.0087, mean loss: 0.0092, :  20%|██        | 44961/224800 [9:03:59<32:33:01,  1.53it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mean loss: 0.0090, \n",
      "saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 20, loss: 0.0085, mean loss: 0.0091, :  21%|██        | 47209/224800 [9:31:09<32:14:38,  1.53it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mean loss: 0.0090, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 21, loss: 0.0107, mean loss: 0.0090, :  22%|██▏       | 49457/224800 [9:58:14<31:38:04,  1.54it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mean loss: 0.0090, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 22, loss: 0.0081, mean loss: 0.0089, :  23%|██▎       | 51705/224800 [10:25:24<31:30:22,  1.53it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mean loss: 0.0089, \n",
      "saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 23, loss: 0.0093, mean loss: 0.0088, :  24%|██▍       | 53953/224800 [10:52:42<31:07:47,  1.52it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mean loss: 0.0087, \n",
      "saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 24, loss: 0.0090, mean loss: 0.0087, :  25%|██▌       | 56201/224800 [11:19:49<30:47:52,  1.52it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mean loss: 0.0091, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 25, loss: 0.0070, mean loss: 0.0087, :  26%|██▌       | 58449/224800 [11:46:53<29:57:24,  1.54it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mean loss: 0.0085, \n",
      "saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 26, loss: 0.0087, mean loss: 0.0086, :  27%|██▋       | 60697/224800 [12:14:00<30:30:02,  1.49it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mean loss: 0.0086, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 27, loss: 0.0102, mean loss: 0.0085, :  28%|██▊       | 62945/224800 [12:41:10<29:21:57,  1.53it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mean loss: 0.0085, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 28, loss: 0.0092, mean loss: 0.0084, :  29%|██▉       | 65193/224800 [13:08:22<31:01:39,  1.43it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mean loss: 0.0084, \n",
      "saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 29, loss: 0.0072, mean loss: 0.0083, :  30%|███       | 67441/224800 [13:35:10<28:05:54,  1.56it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mean loss: 0.0083, \n",
      "saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 30, loss: 0.0098, mean loss: 0.0083, :  31%|███       | 69689/224800 [14:01:56<27:50:06,  1.55it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mean loss: 0.0081, \n",
      "saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 31, loss: 0.0093, mean loss: 0.0082, :  32%|███▏      | 71937/224800 [14:28:44<27:17:16,  1.56it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mean loss: 0.0080, \n",
      "saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 32, loss: 0.0070, mean loss: 0.0082, :  33%|███▎      | 74185/224800 [14:55:31<27:05:13,  1.54it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mean loss: 0.0081, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 33, loss: 0.0077, mean loss: 0.0081, :  34%|███▍      | 76433/224800 [15:22:21<26:34:16,  1.55it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mean loss: 0.0079, \n",
      "saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 34, loss: 0.0076, mean loss: 0.0080, :  35%|███▌      | 78681/224800 [15:49:10<26:09:39,  1.55it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mean loss: 0.0086, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 35, loss: 0.0070, mean loss: 0.0080, :  36%|███▌      | 80929/224800 [16:15:58<25:59:17,  1.54it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mean loss: 0.0080, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 36, loss: 0.0085, mean loss: 0.0079, :  37%|███▋      | 83177/224800 [16:42:47<25:26:40,  1.55it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mean loss: 0.0077, \n",
      "saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 37, loss: 0.0067, mean loss: 0.0079, :  38%|███▊      | 85425/224800 [17:09:36<24:51:23,  1.56it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mean loss: 0.0077, \n",
      "saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 38, loss: 0.0090, mean loss: 0.0078, :  39%|███▉      | 87673/224800 [17:36:29<26:40:15,  1.43it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mean loss: 0.0079, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 39, loss: 0.0072, mean loss: 0.0078, :  39%|███▉      | 87839/224800 [17:39:42<26:05:15,  1.46it/s] "
     ]
    }
   ],
   "source": [
    "## create data generator, train model and save model.\n",
    "BG = BatchGenerator()\n",
    "MT = ModelTrainer()\n",
    "MT.train_full_model()\n",
    "print(\"finished training\")\n",
    "torch.save(MT.strongest_model, model_path + \"model_t1\")\n",
    "model = torch.load(model_path + \"model_t1\")\n",
    "model.eval()\n",
    "print(\"saved the model\")\n",
    "\n",
    "from twilio.rest import Client\n",
    "account_sid = \"ACbb524ad1deed7bd1d53d2be7dd189de6\"\n",
    "auth_token = \"9a0a01cddac5b0c827eb89a534fa897e\"\n",
    "client = Client(account_sid, auth_token)\n",
    "FROM_NUM        = \"+12562861398\"\n",
    "VERIFIED_NUMBER = \"+447722355970\"\n",
    "message = client.messages.create(body='TESTING', from_=[FROM_NUM], to=[VERIFIED_NUMBER])\n",
    "print(message.sid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rapid-newspaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(model_path + \"conv_model\")\n",
    "model.eval()\n",
    "BG = BatchGenerator()\n",
    "train_full_loader, valid_full_loader, test_full_loader = BG.load_full_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "illegal-ethnic",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hearing-concept",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baking-singing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earlier-commonwealth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a sequnce of tactile readings for test data with sample of \n",
    "import cv2\n",
    "criterion1 = nn.L1Loss()\n",
    "criterion2 = nn.MSELoss()\n",
    "\n",
    "experiments = []\n",
    "tactile_predictions = []\n",
    "tactile_groundtruth = []\n",
    "experiment_time_steps = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for index__, batch_features in enumerate(test_full_loader):\n",
    "        # 2. Reshape data and send to device:                    \n",
    "        action = batch_features[0].squeeze().permute(1,0,2).to(device)\n",
    "        tactile = batch_features[1].permute(1,0,4,3,2).to(device)\n",
    "\n",
    "        tp = model.forward(tactiles=tactile, actions=action)\n",
    "        experiments.append(batch_features[2])\n",
    "        experiment_time_steps.append([batch_features[2], batch_features[3]])\n",
    "        \n",
    "        # test for 0 and gt + 10:\n",
    "        image_pred_batch = []\n",
    "        image_gt_batch = []\n",
    "        for image_pred, image_gt in zip(tp[-1], tactile[context_frames-1:][0]):\n",
    "            image_pred_batch.append(cv2.resize(image_pred.permute(1,2,0).cpu().detach().numpy(), dsize=(4,4), interpolation=cv2.INTER_CUBIC).flatten())\n",
    "            image_gt_batch.append(cv2.resize(image_gt.permute(1,2,0).cpu().detach().numpy(), dsize=(4,4), interpolation=cv2.INTER_CUBIC).flatten())\n",
    "        tactile_predictions.append(np.array(image_pred_batch))\n",
    "        tactile_groundtruth.append(np.array(image_gt_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rental-uncertainty",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import (AutoMinorLocator, MultipleLocator)\n",
    "experiment_to_test = 210\n",
    "prediction_data_to_plot = []\n",
    "gt_data_to_plot = []\n",
    "time_stepsto_plot = []\n",
    "for index, (pred_batch, gt_batch) in enumerate(zip(tactile_predictions, tactile_groundtruth)):\n",
    "    for batch_step, (prediction_sample, gt_sample)  in enumerate(zip(pred_batch, gt_batch)):\n",
    "        experiment              = experiment_time_steps[index][0][batch_step][0].numpy()\n",
    "        time_step_in_experiment = experiment_time_steps[index][1][batch_step][0].numpy()\n",
    "        if experiment == experiment_to_test:\n",
    "            prediction_data_to_plot.append(prediction_sample)\n",
    "            gt_data_to_plot.append(gt_sample)\n",
    "            time_stepsto_plot.append(time_step_in_experiment)\n",
    "\n",
    "\n",
    "plot_range_start = 50 # 0\n",
    "plot_range_stop = 100 # -1\n",
    "for tactile in range(48):\n",
    "        fig, ax1 = plt.subplots()\n",
    "        ax1.set_xlabel('time step')\n",
    "        ax1.set_ylabel('tactile reading')\n",
    "        ax1.plot([value for value in time_stepsto_plot[plot_range_start:plot_range_stop]], np.array(prediction_data_to_plot)[plot_range_start:plot_range_stop,tactile], alpha=0.5, c=\"g\", label=\"t10\")\n",
    "        ax1.plot([value for value in time_stepsto_plot[plot_range_start:plot_range_stop]], np.array(gt_data_to_plot)[plot_range_start:plot_range_stop,tactile], alpha=0.5, c=\"r\", label=\"gt\")\n",
    "        ax1.tick_params(axis='y')\n",
    "        fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "        fig.subplots_adjust(top=0.90)\n",
    "        ax1.legend(loc=\"upper right\")\n",
    "\n",
    "        ax1.xaxis.set_minor_locator(AutoMinorLocator())\n",
    "        ax1.tick_params(which='minor', length=1, color='k')\n",
    "        ax1.grid(which='minor')\n",
    "        ax1.grid(which='major')\n",
    "#         ax1.set_ylim((0, 1.0))\n",
    "\n",
    "        plt.title(\"Inline Simple_LSTM tactile \" + str(index))\n",
    "#         plt.savefig(model_path + '/' + str(experiment_to_test) + '/zeros_bigger_pconcat_double_data_model_sample_test_with_loss_' + str(index) + '.png', dpi=300)\n",
    "        plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floating-level",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focal-bolivia",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handed-extension",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excess-infrared",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animated-library",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib.ticker import (AutoMinorLocator, MultipleLocator)\n",
    "# # calculate tactile values for full sample:\n",
    "# time_step_to_test_t1 = 0    # [batch_set, prediction frames(t1->tx)(6), batch_size, features(48)]\n",
    "# time_step_to_test_t5 = 4\n",
    "# time_step_to_test_t9 = 9\n",
    "# predicted_data_t1 = []\n",
    "# predicted_data_t5 = []\n",
    "# predicted_data_t9 = []\n",
    "# groundtruth_data = []\n",
    "# index_test = 0\n",
    "# experiment_to_test = 210\n",
    "# for index, batch_set in enumerate(tactile_predictions):\n",
    "#     for batch in range(0, len(batch_set[0])):\n",
    "#         experiment = experiment_time_steps[index][0][batch][0]\n",
    "#         if experiment == experiment_to_test:\n",
    "#             if not index_test == 0 : ## for use in single prediction plotting \n",
    "#                 single_prediction = batch_set\n",
    "#                 single_ground_truth = tactile_groundtruth[index]\n",
    "#             prediction_values = batch_set[time_step_to_test_t1][batch]\n",
    "#             predicted_data_t1.append(prediction_values)\n",
    "#             prediction_values = batch_set[time_step_to_test_t5][batch]\n",
    "#             predicted_data_t5.append(prediction_values)\n",
    "#             prediction_values = batch_set[time_step_to_test_t9][batch]\n",
    "#             predicted_data_t9.append(prediction_values)\n",
    "#             gt_values = tactile_groundtruth[index][time_step_to_test_t1][batch]\n",
    "#             groundtruth_data.append(gt_values)\n",
    "#             index_test = 1\n",
    "\n",
    "# test data\n",
    "index = 0\n",
    "experiment_to_test = 210\n",
    "titles = [\"sheerx\", \"sheery\", \"normal\"]\n",
    "for j in range(3):\n",
    "    for i in range(16):\n",
    "        groundtruth_taxle = []\n",
    "        predicted_taxel = []\n",
    "        predicted_taxel_t1 = []\n",
    "        predicted_taxel_t5 = []\n",
    "        predicted_taxel_t9 = []\n",
    "        # good = 140, 145 (lifting up the )\n",
    "        for k in range(len(predicted_data_t1)): #320, 420):# len(predicted_data_t1)):#310, 325):#len(predicted_data_t1)):  # add in length of context data\n",
    "            predicted_taxel_t1.append(predicted_data_t1[k][j+i])\n",
    "            predicted_taxel_t5.append(predicted_data_t5[k][j+i])\n",
    "            predicted_taxel_t9.append(predicted_data_t9[k][j+i])\n",
    "            groundtruth_taxle.append(groundtruth_data[k][j+i])\n",
    "\n",
    "        index += 1\n",
    "        fig, ax1 = plt.subplots()\n",
    "        ax1.set_xlabel('time step')\n",
    "        ax1.set_ylabel('tactile reading')\n",
    "        ax1.plot([i for i in predicted_taxel_t1], alpha=0.5, c=\"b\", label=\"t1\")\n",
    "        ax1.plot([i for i in predicted_taxel_t5], alpha=0.5, c=\"k\", label=\"t5\")\n",
    "        ax1.plot([i for i in predicted_taxel_t9], alpha=0.5, c=\"g\", label=\"t10\")\n",
    "        ax1.plot(groundtruth_taxle, alpha=0.5, c=\"r\", label=\"gt\")\n",
    "        ax1.tick_params(axis='y')\n",
    "        fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "        fig.subplots_adjust(top=0.90)\n",
    "        ax1.legend(loc=\"upper right\")\n",
    "        \n",
    "        ax1.xaxis.set_minor_locator(AutoMinorLocator())\n",
    "        ax1.tick_params(which='minor', length=1, color='k')\n",
    "        ax1.grid(which='minor')\n",
    "        ax1.grid(which='major')\n",
    "        ax1.set_ylim((0, 1.0))\n",
    "\n",
    "        plt.title(\"Inline Simple_LSTM tactile \" + str(index))\n",
    "#         plt.savefig(model_path + '/' + str(experiment_to_test) + '/zeros_bigger_pconcat_double_data_model_sample_test_with_loss_' + str(index) + '.png', dpi=300)\n",
    "        plt.show()\n",
    "\n",
    "#         fig, ax1 = plt.subplots()\n",
    "#         ax1.set_xlabel('time step')\n",
    "#         ax1.set_ylabel('tactile reading')\n",
    "#         ax1.plot([i for i in predicted_taxel_t1], alpha=0.5, c=\"b\", label=\"t1\")\n",
    "#         ax1.plot(groundtruth_taxle, alpha=0.5, c=\"r\", label=\"gt\")\n",
    "#         ax1.tick_params(axis='y')\n",
    "#         fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "#         fig.subplots_adjust(top=0.90)\n",
    "#         ax1.legend(loc=\"upper right\")\n",
    "#         plt.title(\"Inline Simple_LSTM tactile \" + str(index))\n",
    "# #         plt.savefig(model_path + '/' + str(experiment_to_test) + '/zeros_bigger_pconcat_double_data_model_T0sample_test_with_loss_' + str(index) + '.png', dpi=300)\n",
    "#         plt.show()\n",
    "        \n",
    "#         fig, ax1 = plt.subplots()\n",
    "#         ax1.set_xlabel('time step')\n",
    "#         ax1.set_ylabel('tactile reading')\n",
    "#         ax1.plot([i for i in predicted_taxel_t5], alpha=0.5, c=\"b\", label=\"t5\")\n",
    "#         ax1.plot(groundtruth_taxle, alpha=0.5, c=\"r\", label=\"gt\")\n",
    "#         ax1.tick_params(axis='y')\n",
    "#         fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "#         fig.subplots_adjust(top=0.90)\n",
    "#         ax1.legend(loc=\"upper right\")\n",
    "#         plt.title(\"Inline Simple_LSTM tactile \" + str(index))\n",
    "# #         plt.savefig(model_path + '/' + str(experiment_to_test) + '/zeros_bigger_pconcat_double_data_model_T5sample_test_with_loss_' + str(index) + '.png', dpi=300)\n",
    "#         plt.show()\n",
    "            \n",
    "#         fig, ax1 = plt.subplots()\n",
    "#         ax1.set_xlabel('time step')\n",
    "#         ax1.set_ylabel('tactile reading')\n",
    "#         ax1.plot([i for i in predicted_taxel_t9], alpha=0.5, c=\"b\", label=\"t10\")\n",
    "#         ax1.plot(groundtruth_taxle, alpha=0.5, c=\"r\", label=\"gt\")\n",
    "#         ax1.tick_params(axis='y')\n",
    "#         fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "#         fig.subplots_adjust(top=0.90)\n",
    "#         ax1.legend(loc=\"upper right\")\n",
    "            \n",
    "#         ax1.xaxis.set_minor_locator(AutoMinorLocator())\n",
    "#         ax1.tick_params(which='minor', length=1, color='k')\n",
    "#         ax1.grid(which='minor')\n",
    "#         ax1.grid(which='major')\n",
    "        \n",
    "#         plt.title(\"Inline Simple_LSTM tactile \" + str(index))\n",
    "# #         plt.savefig(model_path + '/' + str(experiment_to_test) + '/zeros_bigger_pconcat_double_data_pca_model_T9sample_test_with_loss_' + str(index) + '.png', dpi=300)\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "black-parameter",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = single_prediction[:,49,:]\n",
    "ground_truth = single_ground_truth[:,49,:]\n",
    "\n",
    "print(prediction.shape)\n",
    "print(ground_truth.shape)\n",
    "\n",
    "for i in range(48):\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax1.set_xlabel('future time step')\n",
    "    ax1.set_ylabel('tactile reading')\n",
    "    ax1.plot(prediction[:,i], alpha=0.5, c=\"b\", label=\"single prediction\")\n",
    "    ax1.plot(ground_truth[:,i], alpha=0.5, c=\"r\", label=\"gt\")\n",
    "    ax1.tick_params(axis='y')\n",
    "    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "    fig.subplots_adjust(top=0.90)\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MultipleLocator(1))\n",
    "    ax1.grid(which='major')\n",
    "    ax1.set_ylim((0, 1.0))\n",
    "#     plt.savefig(model_path + '/' + str(experiment_to_test) + '/pconcat_double_data_model_single_prediction_sample_' + str(i) + '.png', dpi=300)\n",
    "    plt.title(\"Simple_LSTM for single prediction sequence (0-10), tactile \" + str(i))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organic-footwear",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "federal-pharmacy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "derived-thunder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model on the full test sample:\n",
    "# test model on the full test sample:\n",
    "model = torch.load(model_path + \"conv1_model\")\n",
    "model.eval()\n",
    "BG = BatchGenerator()\n",
    "train_full_loader, valid_full_loader, test_full_loader = BG.load_full_data()\n",
    "\n",
    "criterion1 = nn.L1Loss()\n",
    "criterion2 = nn.MSELoss()\n",
    "\n",
    "test_lossesMAE_x = 0.0\n",
    "test_lossesMSE_x = 0.0\n",
    "test_lossesMAE_y = 0.0\n",
    "test_lossesMSE_y = 0.0\n",
    "test_lossesMAE_z = 0.0\n",
    "test_lossesMSE_z = 0.0\n",
    "\n",
    "test_lossesMAE_t1 = 0.0\n",
    "test_lossesMSE_t1 = 0.0\n",
    "test_lossesMAE_t5 = 0.0\n",
    "test_lossesMSE_t5 = 0.0\n",
    "test_lossesMAE_t10 = 0.0\n",
    "test_lossesMSE_t10 = 0.0\n",
    "\n",
    "test_lossesMAE_x_ts1 = 0.0\n",
    "test_lossesMSE_x_ts1 = 0.0\n",
    "test_lossesMAE_y_ts1 = 0.0\n",
    "test_lossesMSE_y_ts1 = 0.0\n",
    "test_lossesMAE_z_ts1 = 0.0\n",
    "test_lossesMSE_z_ts1 = 0.0\n",
    "\n",
    "test_lossesMAE_x_ts5 = 0.0\n",
    "test_lossesMSE_x_ts5 = 0.0\n",
    "test_lossesMAE_y_ts5 = 0.0\n",
    "test_lossesMSE_y_ts5 = 0.0\n",
    "test_lossesMAE_z_ts5 = 0.0\n",
    "test_lossesMSE_z_ts5 = 0.0\n",
    "\n",
    "test_lossesMAE_x_ts10 = 0.0\n",
    "test_lossesMSE_x_ts10 = 0.0\n",
    "test_lossesMAE_y_ts10 = 0.0\n",
    "test_lossesMSE_y_ts10 = 0.0\n",
    "test_lossesMAE_z_ts10 = 0.0\n",
    "test_lossesMSE_z_ts10 = 0.0\n",
    "\n",
    "experiments = []\n",
    "tactile_predictions = []\n",
    "tactile_groundtruth = []\n",
    "experiment_time_steps = []\n",
    "\n",
    "test_lossesMAE = 0.0\n",
    "test_lossesMSE = 0.0\n",
    "with torch.no_grad():\n",
    "    for index__, batch_features in enumerate(test_full_loader):\n",
    "        # 2. Reshape data and send to device:                    \n",
    "        action = batch_features[0].squeeze().permute(1,0,2).to(device)\n",
    "        tactile = batch_features[1].permute(1,0,4,3,2).to(device)\n",
    "\n",
    "        tp = model.forward(tactiles=tactile, actions=action)\n",
    "        experiments.append(batch_features[2])\n",
    "        experiment_time_steps.append([batch_features[2], batch_features[3]])\n",
    "        tactile_predictions.append(tp)  # Step 3. Run our forward pass.\n",
    "        tactile_groundtruth.append(tactile[context_frames-1:])\n",
    "#         # calculate losses for specific timesteps\n",
    "#         test_lossMAE_t1 = criterion1(tp[0,:,:].to(device), tactile[context_frames:][0,:,:])\n",
    "#         test_lossesMAE_t1 += test_lossMAE_t1.item() \n",
    "#         test_lossMSE_t1 = criterion2(tp[0,:,:].to(device), tactile[context_frames:][0,:,:])\n",
    "#         test_lossesMSE_t1 += test_lossMSE_t1.item() \n",
    "#         test_lossMAE_t5 = criterion1(tp[4,:,:].to(device), tactile[context_frames:][4,:,:])\n",
    "#         test_lossesMAE_t5 += test_lossMAE_t5.item() \n",
    "#         test_lossMSE_t5 = criterion2(tp[4,:,:].to(device), tactile[context_frames:][4,:,:])\n",
    "#         test_lossesMSE_t5 += test_lossMSE_t5.item() \n",
    "#         test_lossMAE_t10 = criterion1(tp[9,:,:].to(device), tactile[context_frames:][9,:,:])\n",
    "#         test_lossesMAE_t10 += test_lossMAE_t10.item() \n",
    "#         test_lossMSE_t10 = criterion2(tp[9,:,:].to(device), tactile[context_frames:][9,:,:])\n",
    "#         test_lossesMSE_t10 += test_lossMSE_t10.item() \n",
    "        \n",
    "#         # calculate losses for specific forces\n",
    "#         test_lossMAE_x = criterion1(tp[:,:,:16].to(device), tactile[context_frames:][:,:,:16])\n",
    "#         test_lossesMAE_x += test_lossMAE_x.item() \n",
    "#         test_lossMSE_x = criterion2(tp[:,:,:16].to(device), tactile[context_frames:][:,:,:16])\n",
    "#         test_lossesMSE_x += test_lossMSE_x.item() \n",
    "#         test_lossMAE_y = criterion1(tp[:,:,17:32].to(device), tactile[context_frames:][:,:,17:32])\n",
    "#         test_lossesMAE_y += test_lossMAE_y.item() \n",
    "#         test_lossMSE_y = criterion2(tp[:,:,17:32].to(device), tactile[context_frames:][:,:,17:32])\n",
    "#         test_lossesMSE_y += test_lossMSE_y.item() \n",
    "#         test_lossMAE_z = criterion1(tp[:,:,33:48].to(device), tactile[context_frames:][:,:,33:48])\n",
    "#         test_lossesMAE_z += test_lossMAE_z.item() \n",
    "#         test_lossMSE_z = criterion2(tp[:,:,33:48].to(device), tactile[context_frames:][:,:,33:48])\n",
    "#         test_lossesMSE_z += test_lossMSE_z.item() \n",
    "\n",
    "#         # calculate losses for specific timesteps and forces \n",
    "#         test_lossMAE_x_ts1 = criterion1(tp[0,:,:16].to(device), tactile[context_frames:][0,:,:16])\n",
    "#         test_lossesMAE_x_ts1 += test_lossMAE_x_ts1.item() \n",
    "#         test_lossMSE_x_ts1 = criterion2(tp[0,:,:16].to(device), tactile[context_frames:][0,:,:16])\n",
    "#         test_lossesMSE_x_ts1 += test_lossMSE_x_ts1.item() \n",
    "#         test_lossMAE_y_ts1 = criterion1(tp[0,:,17:32].to(device), tactile[context_frames:][0,:,17:32])\n",
    "#         test_lossesMAE_y_ts1 += test_lossMAE_y_ts1.item() \n",
    "#         test_lossMSE_y_ts1 = criterion2(tp[0,:,17:32].to(device), tactile[context_frames:][0,:,17:32])\n",
    "#         test_lossesMSE_y_ts1 += test_lossMSE_y_ts1.item() \n",
    "#         test_lossMAE_z_ts1 = criterion1(tp[0,:,33:48].to(device), tactile[context_frames:][0,:,33:48])\n",
    "#         test_lossesMAE_z_ts1 += test_lossMAE_z_ts1.item() \n",
    "#         test_lossMSE_z_ts1 = criterion2(tp[0,:,33:48].to(device), tactile[context_frames:][0,:,33:48])\n",
    "#         test_lossesMSE_z_ts1 += test_lossMSE_z_ts1.item() \n",
    " \n",
    "#         test_lossMAE_x_ts5 = criterion1(tp[4,:,:16].to(device), tactile[context_frames:][4,:,:16])\n",
    "#         test_lossesMAE_x_ts5 += test_lossMAE_x_ts5.item() \n",
    "#         test_lossMSE_x_ts5 = criterion2(tp[4,:,:16].to(device), tactile[context_frames:][4,:,:16])\n",
    "#         test_lossesMSE_x_ts5 += test_lossMSE_x_ts5.item() \n",
    "#         test_lossMAE_y_ts5 = criterion1(tp[4,:,17:32].to(device), tactile[context_frames:][4,:,17:32])\n",
    "#         test_lossesMAE_y_ts5 += test_lossMAE_y_ts5.item() \n",
    "#         test_lossMSE_y_ts5 = criterion2(tp[4,:,17:32].to(device), tactile[context_frames:][4,:,17:32])\n",
    "#         test_lossesMSE_y_ts5 += test_lossMSE_y_ts5.item() \n",
    "#         test_lossMAE_z_ts5 = criterion1(tp[4,:,33:48].to(device), tactile[context_frames:][4,:,33:48])\n",
    "#         test_lossesMAE_z_ts5 += test_lossMAE_z_ts5.item() \n",
    "#         test_lossMSE_z_ts5 = criterion2(tp[4,:,33:48].to(device), tactile[context_frames:][4,:,33:48])\n",
    "#         test_lossesMSE_z_ts5 += test_lossMSE_z_ts5.item() \n",
    "\n",
    "#         test_lossMAE_x_ts10 = criterion1(tp[9,:,:16].to(device), tactile[context_frames:][9,:,:16])\n",
    "#         test_lossesMAE_x_ts10 += test_lossMAE_x_ts10.item() \n",
    "#         test_lossMSE_x_ts10 = criterion2(tp[9,:,:16].to(device), tactile[context_frames:][9,:,:16])\n",
    "#         test_lossesMSE_x_ts10 += test_lossMSE_x_ts10.item() \n",
    "#         test_lossMAE_y_ts10 = criterion1(tp[9,:,17:32].to(device), tactile[context_frames:][9,:,17:32])\n",
    "#         test_lossesMAE_y_ts10 += test_lossMAE_y_ts10.item() \n",
    "#         test_lossMSE_y_ts10 = criterion2(tp[9,:,17:32].to(device), tactile[context_frames:][9,:,17:32])\n",
    "#         test_lossesMSE_y_ts10 += test_lossMSE_y_ts10.item() \n",
    "#         test_lossMAE_z_ts10 = criterion1(tp[9,:,33:48].to(device), tactile[context_frames:][9,:,33:48])\n",
    "#         test_lossesMAE_z_ts10 += test_lossMAE_z_ts10.item() \n",
    "#         test_lossMSE_z_ts10 = criterion2(tp[9,:,33:48].to(device), tactile[context_frames:][9,:,33:48])\n",
    "#         test_lossesMSE_z_ts10 += test_lossMSE_z_ts10.item()\n",
    "\n",
    "# performance_data = []\n",
    "# performance_data.append([\"test loss MAE(L1): \", (test_lossesMAE / index__)])\n",
    "# performance_data.append([\"test loss MSE: \", (test_lossesMSE / index__)])\n",
    "# performance_data.append([\"test loss MAE(L1) timestep 1: \", (test_lossesMAE_x / index__)])\n",
    "# performance_data.append([\"test loss MSE timestep 1: \", (test_lossesMSE_x / index__)])\n",
    "# performance_data.append([\"test loss MAE(L1) timestep 5: \", (test_lossesMAE_y / index__)])\n",
    "# performance_data.append([\"test loss MSE timestep 5: \", (test_lossesMSE_y / index__)])\n",
    "# performance_data.append([\"test loss MAE(L1) timestep 10: \", (test_lossesMAE_z / index__)])\n",
    "# performance_data.append([\"test loss MSE timestep 10: \", (test_lossesMSE_z / index__)])\n",
    "# performance_data.append([\"sheer x test loss MAE(L1): \", (test_lossesMAE_x / index__)])\n",
    "# performance_data.append([\"sheer x test loss MSE: \", (test_lossesMSE_x / index__)])\n",
    "# performance_data.append([\"sheer y test loss MAE(L1): \", (test_lossesMAE_y / index__)])\n",
    "# performance_data.append([\"sheer y test loss MSE: \", (test_lossesMSE_y / index__)])\n",
    "# performance_data.append([\"z test loss MAE(L1): \", (test_lossesMAE_z / index__)])\n",
    "# performance_data.append([\"z test loss MSE: \", (test_lossesMSE_z / index__)])\n",
    "# performance_data.append([\"sheer x test loss MAE(L1) timestep 1: \", (test_lossesMAE_x_ts1 / index__)])\n",
    "# performance_data.append([\"sheer x test loss MSE timestep 1: \", (test_lossesMSE_x_ts1 / index__)])\n",
    "# performance_data.append([\"sheer y test loss MAE(L1) timestep 1: \", (test_lossesMAE_y_ts1 / index__)])\n",
    "# performance_data.append([\"sheer y test loss MSE timestep 1: \", (test_lossesMSE_y_ts1 / index__)])\n",
    "# performance_data.append([\"z test loss MAE(L1) timestep 1: \", (test_lossesMAE_z_ts1 / index__)])\n",
    "# performance_data.append([\"z test loss MSE timestep 1: \", (test_lossesMSE_z_ts1 / index__)])\n",
    "# performance_data.append([\"sheer x test loss MAE(L1) timestep 5: \", (test_lossesMAE_x_ts5 / index__)])\n",
    "# performance_data.append([\"sheer x test loss MSE timestep 5: \", (test_lossesMSE_x_ts5 / index__)])\n",
    "# performance_data.append([\"sheer y test loss MAE(L1) timestep 5: \", (test_lossesMAE_y_ts5 / index__)])\n",
    "# performance_data.append([\"sheer y test loss MSE timestep 5: \", (test_lossesMSE_y_ts5 / index__)])\n",
    "# performance_data.append([\"z test loss MAE(L1) timestep 5: \", (test_lossesMAE_z_ts5 / index__)])\n",
    "# performance_data.append([\"z test loss MSE timestep 5: \", (test_lossesMSE_z_ts5 / index__)])\n",
    "# performance_data.append([\"sheer x test loss MAE(L1) timestep 10: \", (test_lossesMAE_x_ts10 / index__)])\n",
    "# performance_data.append([\"sheer x test loss MSE timestep 10: \", (test_lossesMSE_x_ts10 / index__)])\n",
    "# performance_data.append([\"sheer y test loss MAE(L1) timestep 10: \", (test_lossesMAE_y_ts10 / index__)])\n",
    "# performance_data.append([\"sheer y test loss MSE timestep 10: \", (test_lossesMSE_y_ts10 / index__)])\n",
    "# performance_data.append([\"z test loss MAE(L1) timestep 10: \", (test_lossesMAE_z_ts10 / index__)])\n",
    "# performance_data.append([\"z test loss MSE timestep 10: \", (test_lossesMSE_z_ts10 / index__)])\n",
    "# [print(i) for i in performance_data]\n",
    "\n",
    "# np.save(model_path + 'pp_performance_data_conv1_model', np.asarray(performance_data))\n",
    "# print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-small",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tactile_predictions[0].shape)\n",
    "print(tactile_groundtruth[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-belgium",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.animation as animation\n",
    "import mpl_toolkits.mplot3d.axes3d as p3\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import mpl_toolkits.mplot3d.art3d as art3d\n",
    "from scipy.ndimage.interpolation import map_coordinates\n",
    "\n",
    "# calculate tactile values for full sample:\n",
    "time_step_to_test_t1 = 0    # [batch_set, prediction frames(t1->tx)(6), batch_size, features(48)]\n",
    "time_step_to_test_t5 = 4\n",
    "time_step_to_test_t10 = 9\n",
    "predicted_data_t1 = []\n",
    "predicted_data_t5 = []\n",
    "predicted_data_t10 = []\n",
    "groundtruth_data = []\n",
    "experiment_to_test = 210\n",
    "for index, batch_set in enumerate(tactile_predictions):\n",
    "    for batch in range(0, len(batch_set[0])):\n",
    "        experiment = experiment_time_steps[index][0][batch][0]\n",
    "#         print(experiment)\n",
    "        if experiment == experiment_to_test:\n",
    "            prediction_values = batch_set[time_step_to_test_t1][batch]\n",
    "            predicted_data_t1.append(prediction_values)\n",
    "            prediction_values = batch_set[time_step_to_test_t5][batch]\n",
    "            predicted_data_t5.append(prediction_values)\n",
    "            prediction_values = batch_set[time_step_to_test_t10][batch]\n",
    "            predicted_data_t10.append(prediction_values)\n",
    "            gt_values = tactile_groundtruth[index][time_step_to_test_t1][batch]\n",
    "            groundtruth_data.append(gt_values)\n",
    "\n",
    "class image_player():\n",
    "    def __init__(self, images, save_name, feature):\n",
    "        self.feature = feature\n",
    "        self.save_name = save_name\n",
    "        print(model_path + str(experiment_to_test) + '/' + self.save_name + '_feature_' + str(self.feature) + '.gif')\n",
    "        self.run_the_tape(images)\n",
    "\n",
    "    def grab_frame(self):\n",
    "#         print(self.indexyyy,  end=\"\\r\")\n",
    "#         print(self.images[0].shape)\n",
    "        frame = self.images[self.indexyyy].permute(1,2,0).cpu().detach().numpy()[:,:,self.feature]*255\n",
    "        return frame\n",
    "\n",
    "    def update(self, i):\n",
    "        plt.title(i)\n",
    "        self.im1.set_data(self.grab_frame())\n",
    "        self.indexyyy+=1\n",
    "        if self.indexyyy == len(self.images):\n",
    "            self.indexyyy = 0\n",
    "\n",
    "    def run_the_tape(self, images):\n",
    "        self.indexyyy = 0\n",
    "        self.images = images\n",
    "        ax1 = plt.subplot(1,2,1)\n",
    "        self.im1 = ax1.imshow(self.grab_frame(), cmap='gray', vmin=0, vmax=255)\n",
    "        ani = FuncAnimation(plt.gcf(), self.update, interval=20.8, save_count=len(images), repeat=False)\n",
    "        ani.save(model_path + str(experiment_to_test) + '/' + self.save_name + '_feature_' + str(self.feature) + '.gif')\n",
    "        plt.show()\n",
    "\n",
    "# image_player(predicted_data_t1, \"pp_predicted_data_t1\", 0)\n",
    "# print('1')\n",
    "# image_player(predicted_data_t1, \"pp_predicted_data_t1\", 1)\n",
    "# print('2')\n",
    "# image_player(predicted_data_t1, \"pp_predicted_data_t1\", 2)\n",
    "# print('3')\n",
    "# image_player(predicted_data_t5, \"pp_predicted_data_t5\", 0)\n",
    "# print('4')\n",
    "# image_player(predicted_data_t5, \"pp_predicted_data_t5\", 1)\n",
    "# print('5')\n",
    "# image_player(predicted_data_t5, \"pp_predicted_data_t5\", 2)\n",
    "# print('6')\n",
    "# image_player(predicted_data_t10, \"pp_predicted_data_t10\", 0)\n",
    "# print('7')\n",
    "# image_player(predicted_data_t10, \"pp_predicted_data_t10\", 1)\n",
    "# print('8')\n",
    "# image_player(predicted_data_t10, \"pp_predicted_data_t10\", 2)\n",
    "# print('9')\n",
    "# image_player(groundtruth_data, \"pp_groundtruth_data\", 0)\n",
    "# print('1')\n",
    "# image_player(groundtruth_data, \"pp_groundtruth_data\", 1)\n",
    "# print('2')\n",
    "# image_player(groundtruth_data, \"pp_groundtruth_data\", 2)\n",
    "# print('3')\n",
    "\n",
    "# #save numpy arrays for later analysis:\n",
    "# np_to_save = []\n",
    "# np_predicted_data_t1 = []\n",
    "# np_predicted_data_t5 = []\n",
    "# np_predicted_data_t10 = []\n",
    "# np_groundtruth_data = []\n",
    "# for i in range(0, len(predicted_data_t1)):\n",
    "#     np_predicted_data_t1.append(predicted_data_t1[i].permute(1,2,0).cpu().detach().numpy())\n",
    "#     np_predicted_data_t5.append(predicted_data_t5[i].permute(1,2,0).cpu().detach().numpy())\n",
    "#     np_predicted_data_t10.append(predicted_data_t10[i].permute(1,2,0).cpu().detach().numpy())\n",
    "#     np_groundtruth_data.append(groundtruth_data[i].permute(1,2,0).cpu().detach().numpy())\n",
    "# np.save(model_path + str(experiment_to_test) + '/pp_np_predicted_data_t1_conv1_model', np.asarray(np_predicted_data_t1))\n",
    "# np.save(model_path + str(experiment_to_test) + '/pp_np_predicted_data_t5_conv1_model', np.asarray(np_predicted_data_t5))\n",
    "# np.save(model_path + str(experiment_to_test) + '/pp_np_predicted_data_t10_conv1_model', np.asarray(np_predicted_data_t10))\n",
    "# np.save(model_path + str(experiment_to_test) + '/pp_np_groundtruth_data_conv1_model', np.asarray(np_groundtruth_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missing-clearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "class image_player():\n",
    "    def __init__(self, images, save_name, feature):\n",
    "        self.feature = feature\n",
    "        self.save_name = save_name\n",
    "        print(model_path + str(experiment_to_test) + '/' + self.save_name + '_feature_' + str(self.feature) + '.gif')\n",
    "        self.run_the_tape(images)\n",
    "\n",
    "    def grab_frame(self):\n",
    "#         print(self.indexyyy,  end=\"\\r\")\n",
    "#         print(self.images[0].shape)\n",
    "        frame = self.images[self.indexyyy].permute(1,2,0).cpu().detach().numpy()[:,:,self.feature]*255\n",
    "        return frame\n",
    "\n",
    "    def update(self, i):\n",
    "        plt.title(i)\n",
    "        self.im1.set_data(self.grab_frame())\n",
    "        self.indexyyy+=1\n",
    "        if self.indexyyy == len(self.images):\n",
    "            self.indexyyy = 0\n",
    "\n",
    "    def run_the_tape(self, images):\n",
    "        self.indexyyy = 0\n",
    "        self.images = images\n",
    "        ax1 = plt.subplot(1,2,1)\n",
    "        self.im1 = ax1.imshow(self.grab_frame(), cmap='gray', vmin=0, vmax=255)\n",
    "        ani = FuncAnimation(plt.gcf(), self.update, interval=20.8, save_count=len(images), repeat=False)\n",
    "        ani.save(model_path + str(experiment_to_test) + '/' + self.save_name + '_feature_' + str(self.feature) + '.gif')\n",
    "        plt.show()\n",
    "\n",
    "images = [abs(predicted_data_t10[i] - groundtruth_data[i]) for i in range(len(groundtruth_data))]\n",
    "image_player(images, \"pp_predicted_data_t10_difference\", 2)\n",
    "# image_player(groundtruth_data, \"pp_groundtruth_data\", 2)\n",
    "\n",
    "# #save numpy arrays for later analysis:\n",
    "# np_to_save = []\n",
    "# np_predicted_data_t1 = []\n",
    "# np_predicted_data_t5 = []\n",
    "# np_predicted_data_t10 = []\n",
    "# np_groundtruth_data = []\n",
    "# for i in range(0, len(predicted_data_t1)):\n",
    "#     np_predicted_data_t1.append(predicted_data_t1[i].permute(1,2,0).cpu().detach().numpy())\n",
    "#     np_predicted_data_t5.append(predicted_data_t5[i].permute(1,2,0).cpu().detach().numpy())\n",
    "#     np_predicted_data_t10.append(predicted_data_t10[i].permute(1,2,0).cpu().detach().numpy())\n",
    "#     np_groundtruth_data.append(groundtruth_data[i].permute(1,2,0).cpu().detach().numpy())\n",
    "# np.save(model_path + str(experiment_to_test) + '/pp_np_predicted_data_t1_conv1_model', np.asarray(np_predicted_data_t1))\n",
    "# np.save(model_path + str(experiment_to_test) + '/pp_np_predicted_data_t5_conv1_model', np.asarray(np_predicted_data_t5))\n",
    "# np.save(model_path + str(experiment_to_test) + '/pp_np_predicted_data_t10_conv1_model', np.asarray(np_predicted_data_t10))\n",
    "# np.save(model_path + str(experiment_to_test) + '/pp_np_groundtruth_data_conv1_model', np.asarray(np_groundtruth_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "discrete-nerve",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "index = 0\n",
    "prediction_210 = []\n",
    "ground_truth_210 = []\n",
    "for batch in range(0, len(tactile_predictions)):\n",
    "    for batch_no in range(0, len(tactile_predictions[batch][0])):\n",
    "        if experiments[batch][batch_no][9] == 208:\n",
    "            index += 1\n",
    "            reshape_image_gt = cv2.resize(tactile_groundtruth[batch][0][batch_no].permute(1,2,0).cpu().detach().numpy(), dsize=(4, 4), interpolation=cv2.INTER_CUBIC)\n",
    "            reshape_image_pp = cv2.resize(tactile_predictions[batch][-1][batch_no].permute(1,2,0).cpu().detach().numpy(), dsize=(4, 4), interpolation=cv2.INTER_CUBIC)\n",
    "            ground_truth_210.append(reshape_image_gt)\n",
    "            prediction_210.append(reshape_image_pp)\n",
    "\n",
    "start = 0\n",
    "end   = len(prediction_210)\n",
    "plt.plot([i for i in range(start, end)], [ground_truth_210[i][0][0][2] for i in range(start, end)], 'k')\n",
    "plt.plot([i for i in range(start, end)], [prediction_210[i][0][0][2] for i in range(start, end)], 'r')\n",
    "plt.ylim((0,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crude-absolute",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = 0\n",
    "experi = 0 # not time_step = 1\n",
    "batch_no = 0\n",
    "time_step = 9\n",
    "print(experiment_time_steps[batches][experi][batch_no][time_step].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "later-feelings",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
