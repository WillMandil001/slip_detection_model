{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "golden-forum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import csv\n",
    "import tqdm\n",
    "import copy\n",
    "import click\n",
    "import logging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from string import digits\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "\n",
    "seed = 42\n",
    "epochs = 200\n",
    "batch_size = 128\n",
    "learning_rate = 1e-3\n",
    "context_frames = 10\n",
    "sequence_length = 16\n",
    "lookback = sequence_length\n",
    "\n",
    "context_epochs = 20\n",
    "context_batch_size = 1\n",
    "context_learning_rate = 1e-3\n",
    "context_data_length = 20\n",
    "\n",
    "valid_train_split = 0.8  # precentage of train data from total\n",
    "test_train_split = 0.9  # precentage of train data from total\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")#  use gpu if available\n",
    "################################# CHANGE THIS!!!!  #################################\n",
    "model_path = \"/home/user/Robotics/slip_detection_model/slip_detection_model/manual_data_models/models/conv_model_001/\"\n",
    "################################# CHANGE THIS!!!!  #################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "living-swing",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchGenerator:\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        data_map = []\n",
    "        with open(data_dir + 'map.csv', 'r') as f:  # rb\n",
    "            reader = csv.reader(f)\n",
    "            for row in reader:\n",
    "                data_map.append(row)\n",
    "\n",
    "        if len(data_map) <= 1: # empty or only header\n",
    "            print(\"No file map found\")\n",
    "            exit()\n",
    "\n",
    "        self.data_map = data_map\n",
    "\n",
    "    def load_full_data(self):\n",
    "        dataset_train = FullDataSet(self.data_dir, self.data_map, type_=\"train\")\n",
    "        dataset_valid = FullDataSet(self.data_dir, self.data_map, type_=\"valid\")\n",
    "        dataset_test = FullDataSet(self.data_dir, self.data_map, type_=\"test\")\n",
    "        transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "        train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "        valid_loader = torch.utils.data.DataLoader(dataset_valid, batch_size=batch_size, shuffle=True)\n",
    "        test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size, shuffle=False)\n",
    "        return train_loader, valid_loader, test_loader\n",
    "\n",
    "class FullDataSet():\n",
    "    def __init__(self, data_dir, data_map, type_=\"train\"):\n",
    "        if type_ == \"train\":\n",
    "            self.samples = data_map[1:int(len(data_map)*test_train_split)]\n",
    "        elif type_ == \"valid\":\n",
    "            self.samples = data_map[int(len(data_map)*(valid_train_split)):int(len(data_map)*test_train_split)]\n",
    "        elif type_ == \"test\":\n",
    "            self.samples = data_map[int(len(data_map)*test_train_split):-1]\n",
    "        data_map = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        value = self.samples[idx]\n",
    "        robot = np.load(data_dir + value[0])\n",
    "        xela1image = np.load(data_dir + value[3])\n",
    "        experiment = np.load(data_dir + value[-2])\n",
    "        time_step  = np.load(data_dir + value[-1])     \n",
    "        return([robot.astype(np.float32),\n",
    "                             xela1image.astype(np.float32),\n",
    "                             experiment,\n",
    "                             time_step])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "measured-curtis",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n",
    "        self.bias = bias\n",
    "        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim, out_channels=4 * self.hidden_dim,\n",
    "                              kernel_size=self.kernel_size, padding=self.padding, bias=self.bias)\n",
    "\n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        h_cur, c_cur = cur_state\n",
    "        combined = torch.cat([input_tensor, h_cur], dim=1)  # concatenate along channel axis\n",
    "        combined_conv = self.conv(combined)\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "        c_next = f * c_cur + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "        return h_next, c_next\n",
    "\n",
    "    def init_hidden(self, batch_size, image_size):\n",
    "        height, width = image_size\n",
    "        return (torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device).to(device),\n",
    "                torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device).to(device))\n",
    "\n",
    "class FullModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FullModel, self).__init__()\n",
    "        self.convlstm1 = ConvLSTMCell(input_dim=3, hidden_dim=12, kernel_size=(5, 5), bias=True).cuda()\n",
    "        self.convlstm2 = ConvLSTMCell(input_dim=24, hidden_dim=3, kernel_size=(5, 5), bias=True).cuda()\n",
    "#         self.conv1     = nn.Conv3d(in_channels=24, out_channels=3, kernel_size=5, \n",
    "#                                    stride=1, padding=2, bias=True)\n",
    "\n",
    "    def tile(self, state_action):\n",
    "        tiled_state_action = torch.zeros(self.batch_size, 12, 32, 32)\n",
    "        for batch in range(self.batch_size):\n",
    "            for index, feature in enumerate(state_action[batch]):\n",
    "                tiled_state_action[batch][index][:] = feature\n",
    "        return tiled_state_action\n",
    "\n",
    "    def forward(self, tactiles, actions):\n",
    "        self.batch_size = actions.shape[1]\n",
    "        state = actions[0]\n",
    "        state.to(device)\n",
    "        batch_size__ = tactiles.shape[1]\n",
    "        hidden_1, cell_1 = self.convlstm1.init_hidden(batch_size=self.batch_size, image_size=(32, 32))\n",
    "        hidden_2, cell_2 = self.convlstm2.init_hidden(batch_size=self.batch_size, image_size=(32, 32))\n",
    "        outputs = []\n",
    "        for index, (sample_tactile, sample_action) in enumerate(zip(tactiles.squeeze(), actions.squeeze())):\n",
    "            sample_tactile.to(device)\n",
    "            sample_action.to(device)\n",
    "            # 2. Run through lstm:\n",
    "            if index > context_frames-1:\n",
    "                hidden_1, cell_1 = self.convlstm1(input_tensor=cell_2, cur_state=[hidden_1, cell_1])\n",
    "                state_action = torch.cat((state, sample_action), 1)\n",
    "                state_action_tile = self.tile(state_action).to(device)\n",
    "                robot_and_tactile = torch.cat((state_action_tile.squeeze(), cell_1.squeeze()), 1)\n",
    "                hidden_2, cell_2 = self.convlstm2(input_tensor=robot_and_tactile, cur_state=[hidden_2, cell_2])\n",
    "                outputs.append(cell_2)\n",
    "            else:\n",
    "                hidden_1, cell_1 = self.convlstm1(input_tensor=sample_tactile, cur_state=[hidden_1, cell_1])\n",
    "                state_action = torch.cat((state, sample_action), 1)\n",
    "                state_action_tile = self.tile(state_action).to(device)\n",
    "                robot_and_tactile = torch.cat((state_action_tile.squeeze(), cell_1.squeeze()), 1)\n",
    "                hidden_2, cell_2 = self.convlstm2(input_tensor=robot_and_tactile, cur_state=[hidden_2, cell_2])\n",
    "        return torch.stack(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "careful-allen",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.train_full_loader, self.valid_full_loader, self.test_full_loader = BG.load_full_data()\n",
    "        self.full_model = FullModel()\n",
    "        self.criterion = nn.L1Loss()\n",
    "        self.optimizer = optim.Adam(self.full_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    def train_full_model(self):\n",
    "        best_model_loss_score = 10.0 \n",
    "        self.plot_training_loss = []\n",
    "        self.plot_validation_loss = []\n",
    "        previous_val_mean_loss = 1.0\n",
    "        early_stop_clock = 0\n",
    "        progress_bar = tqdm.tqdm(range(0, epochs), total=(epochs*len(self.train_full_loader)))\n",
    "        mean_test = 0\n",
    "        for epoch in progress_bar:\n",
    "            loss = 0\n",
    "            losses = 0.0\n",
    "            for index, batch_features in enumerate(self.train_full_loader):\n",
    "                action  = batch_features[0].permute(1,0,2).to(device)\n",
    "                tactile = batch_features[1].permute(1,0,4,2,3).to(device)\n",
    "                tactile_predictions = self.full_model.forward(tactiles=tactile, actions=action) # Step 3. Run our forward pass.\n",
    "                self.optimizer.zero_grad()\n",
    "                loss = self.criterion(tactile_predictions.to(device), tactile[context_frames:])\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                losses += loss.item()\n",
    "                if index:\n",
    "                    mean = losses / index\n",
    "                else:\n",
    "                    mean = 0\n",
    "                progress_bar.set_description(\"epoch: {}, \".format(epoch) + \"loss: {:.4f}, \".format(float(loss.item())) + \"mean loss: {:.4f}, \".format(mean))\n",
    "                progress_bar.update()\n",
    "                self.plot_training_loss.append(mean)\n",
    "\n",
    "            val_losses = 0.0\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for index__, batch_features in enumerate(self.valid_full_loader):\n",
    "                    action = batch_features[0].permute(1,0,2).to(device)\n",
    "                    tactile = batch_features[1].permute(1,0,4,2,3).to(device)\n",
    "                    tactile_predictions = self.full_model.forward(tactiles=tactile, actions=action)  # Step 3. Run our forward pass.\n",
    "                    self.optimizer.zero_grad()\n",
    "                    val_loss = self.criterion(tactile_predictions.to(device), tactile[context_frames:])\n",
    "                    val_losses += val_loss.item()\n",
    "\n",
    "            print(\"Validation mean loss: {:.4f}, \".format(val_losses / index__))\n",
    "            self.plot_validation_loss.append(val_losses / index__)\n",
    "            if previous_val_mean_loss < val_losses / index__:\n",
    "                early_stop_clock +=1\n",
    "                previous_val_mean_loss = val_losses / index__ \n",
    "                if early_stop_clock == 3:\n",
    "                    print(\"Early stopping\")\n",
    "                    break\n",
    "            else:\n",
    "                if (val_losses / index__) < best_model_loss_score:\n",
    "                    self.strongest_model = copy.deepcopy(self.full_model)\n",
    "                early_stop_clock = 0\n",
    "                previous_val_mean_loss = val_losses / index__\n",
    "        plt.plot(self.plot_training_loss, c=\"r\", label=\"train loss MAE\")\n",
    "        plt.plot(self.plot_validation_loss, c='b', label=\"val loss MAE\")\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.show()\n",
    "        plt.savefig(model_path + '/model_training_plot.png', dpi=300)\n",
    "        np.save(model_path + 'training_loss', np.asarray(self.plot_training_loss))\n",
    "        np.save(model_path + 'validation_loss', np.asarray(self.plot_validation_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "steady-venice",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "data_dir = '/home/user/Robotics/Data_sets/slip_detection/manual_slip_detection/'\n",
    "BG = BatchGenerator(data_dir)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "activated-reply",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.0255, mean loss: 0.0455, :   1%|          | 712/142200 [23:32<1684:55:09, 42.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mean loss: 0.0245, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 0.0240, mean loss: 0.0253, :   1%|          | 1423/142200 [46:27<89:38:04,  2.29s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mean loss: 0.0214, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 2, loss: 0.0275, mean loss: 0.0240, :   1%|          | 1545/142200 [52:41<64:57:19,  1.66s/it]  "
     ]
    }
   ],
   "source": [
    "MT = ModelTrainer(data_dir)\n",
    "MT.train_full_model()\n",
    "print(\"finished training\")\n",
    "torch.save(MT.strongest_model, model_path + \"full_model\")\n",
    "model = torch.load(model_path + \"full_model\")\n",
    "model.eval()\n",
    "print(\"saved the model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynamic-theorem",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternate-article",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "written-suicide",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fluid-mouse",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innovative-pointer",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
